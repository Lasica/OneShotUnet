{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from commons import *\n",
    "from gan_arch import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No compatible GPUs found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METU_RAW_PATH = '/qarr/studia/magister/datasets/METU/930k_logo_v3/'\n",
    "METU_DATASET_PATH = '/home/zenfur/magister/resized_930k_logo/'\n",
    "EVAL_ORIGIN_PATH = '/qarr/studia/magister/datasets/METU/query_reversed/'\n",
    "EVAL_DATASET_PATH = '/home/zenfur/magister/metu_eval_256sq/'\n",
    "LOG_DIR = \"siamese_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_SAVE_NAME = \"siamese_model_1\"\n",
    "TESTING=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset pipeline and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesList = tf.io.matching_files(EVAL_DATASET_PATH + \"*.jpg\")\n",
    "\n",
    "@tf.function\n",
    "def tf_get_filename(path):\n",
    "    return tf.strings.regex_replace(path, \"[^/]*/\", \"\")\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def tf_read_image(path):\n",
    "    # Retrieving the group number from file name\n",
    "    img = tf.io.read_file(path)\n",
    "    return tf.image.decode_jpeg(img, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "\n",
    "\n",
    "def tf_get_class_from_name(path):\n",
    "    filename = tf_get_filename(path)\n",
    "    group_number = tf.strings.to_number(\n",
    "        tf.strings.regex_replace(filename, \"-.*$\", \"\"), \n",
    "        out_type=tf.dtypes.int32\n",
    "    )\n",
    "    return group_number\n",
    "\n",
    "#@tf.function\n",
    "def tf_convert_and_normalize_img(img):\n",
    "    c = tf.constant(256.0, dtype=tf.dtypes.float32)\n",
    "    img = tf.cast(img, tf.dtypes.float32)\n",
    "    return tf.math.divide(img, c)\n",
    "\n",
    "\n",
    "evalpathsDB = tf.data.Dataset.from_tensor_slices(imagesList)\n",
    "\n",
    "DBlen = len(imagesList)\n",
    "\n",
    "evalDB = (      evalpathsDB.map(tf_read_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                 .batch(32)\n",
    "                 .map(tf_convert_and_normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "evalGrps = (      evalpathsDB.map(tf_get_class_from_name, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                   .batch(32)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting and preparing images into groups by name\n",
    "By convention, discarding images from group 0, as they have been manually inserted as the examples that differ from the rest sampled from 930k METU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = list(evalGrps.unbatch().as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageGroupsDct = dict()\n",
    "for i, l in enumerate(labs):\n",
    "    last_count = imageGroupsDct.get(l,(i,0))\n",
    "    imageGroupsDct[l] = (last_count[0], last_count[1] + 1)\n",
    "del(imageGroupsDct[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageGroups = list(imageGroupsDct.items())\n",
    "avoids, starts, lengths, seeds = [], [], [], []\n",
    "\n",
    "# Generating the random seeds for custom random sequence generators that iterate over triplets from each group\n",
    "# For the sake of being repeatable, fixing seed\n",
    "np.random.seed(949127843)\n",
    "for igrp in imageGroups:\n",
    "    for i in range(igrp[1][1]):\n",
    "        avoids.append(i+igrp[1][0])\n",
    "        starts.append(igrp[1][0])\n",
    "        lengths.append(igrp[1][1])\n",
    "        seeds.append(np.random.randint(0, high=10000000))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom triples sampler based on linear congruent generator \n",
    "The generator has period equal NumberOfUnlikeSamples*(NumberOfAlikeSamples-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_generator(start, length, avoid, n, initial, prime=756212269):\n",
    "    \"\"\"\n",
    "    results in sequence of triplets (<start:start+length>, <avoid>, <0:n-1 excluding start:start+length-1>)\n",
    "    \"\"\"\n",
    "    start = np.array(start, dtype=np.int32)\n",
    "    length = np.array(length, dtype=np.int32)\n",
    "    avoid = np.array(avoid, dtype=np.int32)\n",
    "    initial = np.array(initial, dtype=np.int32)\n",
    "    current = initial\n",
    "    unlikeCount = (n-length)\n",
    "    length = length - 1\n",
    "    modulo_base = length * unlikeCount\n",
    "    multiplier = modulo_base*11*3+1\n",
    "    while True:\n",
    "        current = (multiplier * current + prime) % modulo_base\n",
    "        like = (current % length) + start\n",
    "        unlike = current // length\n",
    "        like = like + (like >= avoid)\n",
    "        unlike = unlike + ((unlike >= start) * (length+1))\n",
    "        like = np.expand_dims(like, axis=1)\n",
    "        unlike = np.expand_dims(unlike, axis=1)\n",
    "        for triplet in np.concatenate((like, np.expand_dims(avoid, axis=1), unlike), axis=1):\n",
    "            yield triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = triplet_generator(starts, lengths, avoids, len(labs), seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    # Test: group 1:3, n=6, x = 2\n",
    "    g = triplet_generator([1], [3], [2], 6, [423432231])\n",
    "    s = set()\n",
    "    for i in range(2*3):\n",
    "        t = next(g)\n",
    "        print(t)\n",
    "        if tuple(t) in s:\n",
    "            break\n",
    "        else:\n",
    "            s.add(tuple(t))\n",
    "    assert i == (2*3)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    for i in range(10):\n",
    "        print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline segments \n",
    "\n",
    "Converting the database of samples into numpy array, since it can fit into RAM memory to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesTable = tf_db_to_array(evalDB, DBlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_by_index(triplet):\n",
    "    # @triplet: tuple/tensor of indices in the images table\n",
    "    return tf.gather(imagesTable, triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the common source of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(41431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data augmentation functions for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_slice_224x224(image, seed):\n",
    "    return tf.image.stateless_random_crop(image, [224,224,3], seed)\n",
    "\n",
    "\n",
    "def random_rotate(image, seed):\n",
    "    if seed > 3:\n",
    "        seed = 0\n",
    "    return tf.image.rot90(image, k=seed)\n",
    "    \n",
    "    \n",
    "def augment(image):\n",
    "    seeds = [rng.make_seeds(2)[0], rng.uniform([], minval=0, maxval=5, dtype=tf.int32)]\n",
    "    sditer = iter(seeds)\n",
    "    \n",
    "    image = random_slice_224x224(image, next(sditer))\n",
    "    \n",
    "    image = random_rotate(image, next(sditer))\n",
    "    \n",
    "    return image\n",
    "\n",
    "# @tf.function\n",
    "# def random_slice_224x224(img, seed):\n",
    "#     # TODO: get zipped random input to make it deterministic and reproducable\n",
    "#     # assuming 256x256x3 size\n",
    "#     randomCrop = tf.concat([tf.random.uniform((2,), minval=0, maxval=256-224, dtype=tf.dtypes.int32), \n",
    "#                            tf.constant([0])], axis=0)\n",
    "#     return tf.slice(img, randomCrop, [224,224,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 3\n",
    "df = (tf.data.Dataset.from_generator(triplet_generator,\n",
    "                                    args = [starts, lengths, avoids, len(labs), seeds],\n",
    "                                    output_signature=tf.TensorSpec(shape=(3), dtype=tf.int32))\n",
    "      .shuffle(256)\n",
    "      .map(get_triplet_by_index, num_parallel_calls=AUTOTUNE)\n",
    "      .unbatch()\n",
    "      .map(augment, num_parallel_calls=AUTOTUNE)#, deterministic=True) # transforms of data augmentation, should be deterministic...\n",
    "      .batch(3*BATCH_SIZE)\n",
    "      .prefetch(2)#AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling and testing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    img = get_triplet_by_index(tf.constant(next(triplet_generator(starts, lengths, avoids, len(labs), seeds))))\n",
    "    f, subplots = plt.subplots(1,3)\n",
    "    for i, sb in enumerate(subplots):\n",
    "        sb.imshow(img[i])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING or True:\n",
    "    img = df.take(1)\n",
    "    img = next(iter(img))\n",
    "    f, subplots = plt.subplots(1,3)\n",
    "    for i, sb in enumerate(subplots):\n",
    "        sb.imshow(img[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese model\n",
    "\n",
    "Importing the  pre-trained VGG16 model with weights from imagenet without classification part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2 dense layers on top of convolutions for 4096 representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3),\n",
    "#     input_shape=None,\n",
    "#     pooling=None,\n",
    ")\n",
    "\n",
    "# TODO: load model if saved is found\n",
    "siamese_base = tf.keras.models.Sequential()\n",
    "for layer in (vgg16,\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(4096, activation='relu'),\n",
    "                tf.keras.layers.Dense(4096, activation='relu')\n",
    "             ):\n",
    "    siamese_base.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    tf.keras.utils.plot_model(vgg16, \"vgg16.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch=32\n",
    "# input_alike = tf.keras.layers.Input(shape=(224,224,3), batch_size=batch, name=\"alike_anchor\")\n",
    "# input_anchor = tf.keras.layers.Input(shape=(224,224,3), batch_size=batch, name=\"anchor\")\n",
    "# input_unlike = tf.keras.layers.Input(shape=(224,224,3), batch_size=batch, name=\"unlike_anchor\")\n",
    "\n",
    "# alike_triplet = siamese_base(input_alike)\n",
    "# anchor_triplet = siamese_base(input_anchor)\n",
    "# unlike_triplet = siamese_base(input_unlike)\n",
    "\n",
    "# # triplet_siamese = tf.keras.models.Model(inputs=[input_alike, input_anchor, input_unlike], \n",
    "# #                                         outputs=[alike_triplet, anchor_triplet, unlike_triplet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese triplet loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(alike, anchor, unlike, margin=1.0):\n",
    "#     together = tf.concat([alike, anchor, unlike], axis=0)\n",
    "#     maxims = tf.math.reduce_max(together, axis=0)\n",
    "#     minims = tf.math.reduce_min(together, axis=0)\n",
    "#     difference = maxims-minims\n",
    "#     alike = (alike - minims)/difference\n",
    "#     anchor = (anchor - minims)/difference\n",
    "#     unlike = (unlike - minims)/difference\n",
    "    a = tf.norm(alike-anchor)\n",
    "    b = tf.norm(unlike-anchor)\n",
    "    return tf.maximum(a + margin - b, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSiamese(tf.keras.models.Model):\n",
    "    def __init__(self, shared_net):\n",
    "        super(TripletSiamese, self).__init__()\n",
    "        self.siamese_base = shared_net\n",
    "\n",
    "    def compile(self, optimizer, loss_margin):\n",
    "        super(TripletSiamese, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = triplet_loss#lambda a,b,c: triplet_loss(a,b,c, margin=loss_margin)\n",
    "\n",
    "    def train_step(self, input_triplets):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # training and calculating the error function gradient\n",
    "            representations = self.siamese_base(input_triplets, training=True)\n",
    "            #representations = tf.keras.utils.normalize(representations, axis=1)\n",
    "            alike = tf.strided_slice(representations, [0,0], tf.shape(representations), strides=[3,1])\n",
    "            anchor = tf.strided_slice(representations, [1,0], tf.shape(representations), strides=[3,1])\n",
    "            unlike = tf.strided_slice(representations, [2,0], tf.shape(representations), strides=[3,1])\n",
    "            loss = self.loss(alike, anchor, unlike)\n",
    "        grads = tape.gradient(loss, self.siamese_base.trainable_weights)\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             alike = self.siamese_base(input_triplets[0], training=True)\n",
    "#         grads1 = tape.gradient(alike, self.siamese_base.trainable_weights) \n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             anchor = self.siamese_base(input_triplets[1], training=True)\n",
    "#         grads2 = tape.gradient(anchor, self.siamese_base.trainable_weights) \n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             unlike = self.siamese_base(input_triplets[2], training=True)\n",
    "#         grads3 = tape.gradient(unlike, self.siamese_base.trainable_weights) \n",
    "        \n",
    "#         grads1 *= 2*(alike - anchor)\n",
    "#         grads2 *= 2*(unlike - alike)\n",
    "#         grads3 *= 2*(alike - unlike)\n",
    "        \n",
    "#         grads = np.mean([grads1, grads2, grads3])# ...\n",
    "            \n",
    "        self.optimizer.apply_gradients(zip(grads, self.siamese_base.trainable_weights))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = TripletSiamese(siamese_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "triplet_model.compile(opt, triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add saving/loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the training with hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.sum([a[1][1] for a in imageGroupsDct.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = LOG_DIR)#, histogram_freq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train\n",
    "train_for = 200\n",
    "try:\n",
    "    triplet_model.fit(df, \n",
    "                  initial_epoch=0,\n",
    "                  epochs=3,#train_for, \n",
    "                  steps_per_epoch=N//BATCH_SIZE,\n",
    "                  callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "                 ) # batch_size unspecified since it's generated by generator\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Interrupted\")\n",
    "    \n",
    "if MODEL_SAVE_NAME is not None:\n",
    "    triplet_model.save(MODEL_SAVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if MODEL_SAVE_NAME is not None:\n",
    "    triplet_model.save(MODEL_SAVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
