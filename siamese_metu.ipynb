{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Imports and setting up environment\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from commons import *\n",
    "from gan_arch import *\n",
    "from datetime import datetime\n",
    "import metuhelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No compatible GPUs found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Constants and globals\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"debuggingExperiment\"\n",
    "DATETIME =  datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXPERIMENT_NAME = \"_\".join([EXPERIMENT_NAME, DATETIME])\n",
    "\n",
    "METU_RAW_PATH = '/qarr/studia/magister/datasets/METU/930k_logo_v3/'\n",
    "METU_DATASET_PATH = '/home/zenfur/magister/resized_930k_logo/'\n",
    "EVAL_ORIGIN_PATH = '/qarr/studia/magister/datasets/METU/query_reversed/'\n",
    "EVAL_DATASET_PATH = '/home/zenfur/magister/metu_eval_256sq/'\n",
    "LOG_DIR = \"siamese_logs/\" \n",
    "MODEL_DIR = \"model_checkpoints/\"\n",
    "LAST_MODEL = \"/home/zenfur/magister/jupyter/siamese_model20210322-032225_1560/\"\n",
    "ANALYSIS_DIR = \"plots/\"\n",
    "\n",
    "LOAD = False\n",
    "TESTING = False\n",
    "HIDE_WARNINGS = True\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TRUE_BATCH_SIZE = BATCH_SIZE * 3\n",
    "lastEpoch=1\n",
    "\n",
    "# lastSample = tf.Variable(tf.zeros([24, 224, 224, 3], dtype=tf.dtypes.float64))\n",
    "# lastModel = [tf.Variable(tf.zeros([25088,2], dtype=tf.dtypes.float32)), tf.Variable(tf.zeros([2], dtype=tf.dtypes.float32))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.experimental.enable_dump_debug_info(LOG_DIR, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_experiment_name(name):\n",
    "    global EXPERIMENT_NAME, DATETIME\n",
    "    EXPERIMENT_NAME = name\n",
    "    DATETIME = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    EXPERIMENT_NAME = \"_\".join([EXPERIMENT_NAME, DATETIME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Functions declarations and definitions\n",
    "---\n",
    "---\n",
    "\n",
    "### Definitions of pipeline building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_get_filename(path):\n",
    "    return tf.strings.regex_replace(path, \"[^/]*/\", \"\")\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def tf_read_image(path):\n",
    "    # Retrieving the group number from file name\n",
    "    img = tf.io.read_file(path)\n",
    "    return tf.image.decode_jpeg(img, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "\n",
    "\n",
    "def tf_get_class_from_name(path):\n",
    "    filename = tf_get_filename(path)\n",
    "    group_number = tf.strings.to_number(\n",
    "        tf.strings.regex_replace(filename, \"-.*$\", \"\"), \n",
    "        out_type=tf.dtypes.int32\n",
    "    )\n",
    "    return group_number\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def tf_convert_and_normalize_img(img):\n",
    "    c = tf.constant(256.0, dtype=tf.dtypes.float32)\n",
    "    img = tf.cast(img, tf.dtypes.float32)\n",
    "    return tf.math.divide(img, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_getter(reference_source):\n",
    "    def get_triplet_by_index(triplet):\n",
    "        # @triplet: tuple/tensor of indices in the images table\n",
    "        return tf.gather(reference_source, triplet)\n",
    "    return get_triplet_by_index\n",
    "\n",
    "\n",
    "def translator_getter(translationTable):\n",
    "    def translate_indices(triplet):\n",
    "        return tf.gather(translationTable, triplet)\n",
    "    return translate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_slice_224x224(image, seed):\n",
    "    return tf.image.stateless_random_crop(image, [224,224,3], seed)\n",
    "\n",
    "\n",
    "def get_center_slice(image):\n",
    "    return tf.image.central_crop(image, 224/256)\n",
    "\n",
    "\n",
    "def random_rotate(image, seed):\n",
    "    if seed > 3:\n",
    "        seed = 0\n",
    "    return tf.image.rot90(image, k=seed)\n",
    "\n",
    "\n",
    "def static_augment(image, seeds):\n",
    "    sditer = iter(seeds)\n",
    "    \n",
    "    image = random_slice_224x224(image, next(sditer))\n",
    "    image = random_rotate(image, next(sditer))\n",
    "    \n",
    "    return image\n",
    "    \n",
    "    \n",
    "def augmenter_getter(rng):\n",
    "    def augment(image):\n",
    "        seeds = [rng.make_seeds(2)[0], rng.uniform([], minval=0, maxval=5, dtype=tf.int32)]\n",
    "        return static_augment(image, seeds)\n",
    "    return augment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom triples sampler based on linear congruent generator \n",
    "The generator has period equal NumberOfUnlikeSamples*(NumberOfAlikeSamples-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_generator(start, length, avoid, n, initial, prime=756212269):\n",
    "    \"\"\"\n",
    "    results in sequence of triplets (<start:start+length>, <avoid>, <0:n-1 excluding start:start+length-1>)\n",
    "    \"\"\"\n",
    "    start = np.array(start, dtype=np.int32)\n",
    "    length = np.array(length, dtype=np.int32)\n",
    "    avoid = np.array(avoid, dtype=np.int32)\n",
    "    initial = np.array(initial, dtype=np.int32)\n",
    "    current = initial\n",
    "    unlikeCount = (n-length)\n",
    "    length = length - 1\n",
    "    modulo_base = length * unlikeCount\n",
    "    multiplier = modulo_base*11*3+1\n",
    "    while True:\n",
    "        # Generating next modulo from sequence\n",
    "        current = (multiplier * current + prime) % modulo_base\n",
    "        like = (current % length)\n",
    "        unlike = current // length\n",
    "        \n",
    "        # Calculating proper indices from random modulos\n",
    "        like += start\n",
    "        like += (like >= avoid)\n",
    "        like = np.expand_dims(like, axis=1)\n",
    "        \n",
    "        unlike = unlike + ((unlike >= start) * (length+1))\n",
    "        unlike = np.expand_dims(unlike, axis=1)\n",
    "        \n",
    "        for triplet in np.concatenate((like, np.expand_dims(avoid, axis=1), unlike), axis=1):\n",
    "            yield triplet\n",
    "\n",
    "def advance_triplet_generator(steps, length, n, initial, prime=756212269):\n",
    "    length = np.array(length, dtype=np.int32)\n",
    "    initial = np.array(initial, dtype=np.int32)\n",
    "    steps = steps // n\n",
    "    current = initial\n",
    "    unlikeCount = (n-length)\n",
    "    length = length - 1\n",
    "    modulo_base = length * unlikeCount\n",
    "    multiplier = modulo_base*11*3+1\n",
    "    for i in range(steps):\n",
    "        current = (multiplier * current + prime) % modulo_base\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(selected_images, representations, scale=0.5, samples_per_image=5):\n",
    "    \"\"\"\n",
    "    Function for drawing sheet of \n",
    "    \"\"\"\n",
    "    samples_per_image += 1\n",
    "    fig, plots = plt.subplots(len(selected_images), \n",
    "                              samples_per_image, \n",
    "                              figsize=(80/12*scale/6*samples_per_image,130*len(selected_images)/120*scale)\n",
    "                             )\n",
    "    for i, selected in enumerate(selected_images):\n",
    "        img = representations[selected]\n",
    "        dist = lambda x: tf.sqrt(tf.reduce_sum((x - img)**2))\n",
    "        reprs_distance = tf.map_fn(dist, representations)\n",
    "        closest_idx = np.argsort(reprs_distance)\n",
    "        for j, p in enumerate(closest_idx[0:samples_per_image]):\n",
    "            plots[i][j].imshow(imagesTable[p].numpy())\n",
    "            if j == 0:\n",
    "                plots[i][j].set_title(f\"{p}\", fontsize=7)\n",
    "            else:\n",
    "                plots[i][j].set_title(f\"{reprs_distance[p]:2.3}\", fontsize=7)\n",
    "            plots[i][j].axes.get_xaxis().set_visible(False)\n",
    "            plots[i][j].axes.get_yaxis().set_visible(False)\n",
    "            \n",
    "    return fig, plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom tensorboard callback for logging and saving training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanTBCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, name=\"\", previewInterval=0, drawSamplesList=None, *args, **kwargs):\n",
    "        super(MeanTBCallback, self).__init__(*args, **kwargs)\n",
    "        self.name = name or DATETIME\n",
    "        self.previewInterval = previewInterval\n",
    "        self.selectedToDraw = drawSamplesList or list()\n",
    "        self.mean_train_loss = 0\n",
    "        self.mean_test_loss = 0\n",
    "        self.train_batches = 0\n",
    "        self.test_batches = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(MeanTBCallback, self).on_epoch_end(epoch, logs=logs)\n",
    "        global imagesTable\n",
    "        # Tensorflow 2.2.0 breaks backwards compatibility here\n",
    "        writer = self._train_writer\n",
    "        \n",
    "        if self.train_batches > 0:\n",
    "            with self._train_writer.as_default():\n",
    "                tf.summary.scalar(\"mean_loss\", self.mean_train_loss/self.train_batches, step = epoch)\n",
    "\n",
    "        if self.test_batches > 0:\n",
    "            tf.print(f\"Mean losses: train:{self.mean_train_loss/self.train_batches} val:{self.mean_test_loss/self.test_batches}\")\n",
    "            with self._val_writer.as_default():\n",
    "                tf.summary.scalar(\"mean_loss\", self.mean_test_loss/self.test_batches, step = epoch)\n",
    "\n",
    "        # TODO split functionality into separate callbacks?\n",
    "        if self.previewInterval > 0 and epoch % self.previewInterval == 0:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Saving the images plot\n",
    "            tf.print(f\"Saving sampling drawings {EXPERIMENT_NAME + '_' + str(epoch)}\")\n",
    "            reprs = self.model.predict(tf.image.central_crop(imagesTable, 224/256))\n",
    "            norm_reps = self.model.normalize_output(reprs)\n",
    "            f, p = plot_results(self.selectedToDraw, norm_reps, scale=1.0)\n",
    "            f.savefig(f\"{ANALYSIS_DIR}{EXPERIMENT_NAME}_{epoch}.png\")\n",
    "            del f\n",
    "            del p\n",
    "            plt.close()\n",
    "            \n",
    "            # Calculating ranks\n",
    "            raw_rank = metuhelpers.calculate_rank(norm_reps, \n",
    "                                                  norm_reps, \n",
    "                                                  {k:[i for i in range(v[0], v[0]+v[1])] for k,v in imageGroups},\n",
    "                                                  metuhelpers.euclidean_sq)\n",
    "\n",
    "            ranks = np.mean([np.mean(x+0.5) for x in raw_rank.values() if x is not []])\n",
    "            ranks_normalised = np.mean([(np.mean(x+0.5)-(len(x)+1)/2)/len(norm_reps) for x in raw_rank.values() if x is not []])\n",
    "\n",
    "            # Logging the ranks\n",
    "            self.log_value(ranks, \"rank\", epoch)\n",
    "            self.log_value(ranks_normalised, \"rank_normalised\", epoch)\n",
    "            tf.print(f\"Rank {ranks:2.4f} Normalised: {ranks_normalised:2.4f}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            tf.print(f'Time taken for evaluation and saving: {end_time-start_time} seconds')\n",
    "\n",
    "        self.train_batches = 0\n",
    "        self.test_batches = 0\n",
    "        self.mean_train_loss = 0\n",
    "        self.mean_test_loss = 0\n",
    "        \n",
    "        \n",
    "    #@tf.function\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.train_batches += 1\n",
    "        if \"loss\" in logs:\n",
    "            self.mean_train_loss += logs[\"loss\"]\n",
    "\n",
    "    #@tf.function\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.test_batches += 1\n",
    "        if \"loss\" in logs:\n",
    "            self.mean_test_loss += logs[\"loss\"]\n",
    "            \n",
    "    def log_value(self, value, value_name, epoch):\n",
    "        with self._train_writer.as_default():\n",
    "            tf.summary.scalar(value_name, value, step = epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSaverCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_interval):\n",
    "        self.save_interval = save_interval\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Saving the model\n",
    "        if self.save_interval > 0 and epoch % self.save_interval == 0:\n",
    "            tf.print(f\"Saving the model {EXPERIMENT_NAME + '_' + str(epoch)}\")\n",
    "            self.model.save(MODEL_DIR + EXPERIMENT_NAME + \"_\" + str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese triplet loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(alike, anchor, unlike, margin=1.0, reduce=tf.reduce_mean):\n",
    "    a = tf.reduce_sum(tf.math.squared_difference(alike, anchor), axis=1)\n",
    "    b = tf.reduce_sum(tf.math.squared_difference(unlike, anchor), axis=1)\n",
    "    return reduce(tf.maximum(a + margin - b, 0.0))\n",
    "\n",
    "# def triplet_loss(alike, anchor, unlike, margin=1.0, reduce=tf.reduce_mean):\n",
    "#     a = tf.norm(alike-anchor, axis=1)\n",
    "#     b = tf.norm(unlike-anchor, axis=1)\n",
    "#     return reduce(tf.maximum(a + margin - b, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom triplet model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_output_with_max(x):\n",
    "    return x / tf.expand_dims(tf.maximum(tf.math.reduce_max(x, axis=1), 1e-5), axis=1)\n",
    "\n",
    "def normalize_output_with_max_batch(x):\n",
    "    return x / tf.maximum(tf.math.reduce_max(x, axis=None), 1e-5)\n",
    "\n",
    "def normalize_output_with_norm(x):\n",
    "    return x / tf.expand_dims(tf.maximum(tf.norm(x, axis=1), 1e-5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero64 = tf.constant(0.0, dtype=tf.dtypes.float64)\n",
    "zero32 = tf.constant(0.0, dtype=tf.dtypes.float32)\n",
    "class TripletSiamese(tf.keras.models.Model):\n",
    "    def __init__(self, shared_net, output_normalizer, name=None):\n",
    "        super(TripletSiamese, self).__init__(name=name)\n",
    "        self.siameseBase = shared_net\n",
    "        self.normalize_output = output_normalizer\n",
    "        self.callctr = 0\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        super(TripletSiamese, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss#lambda a,b,c: triplet_loss(a,b,c, margin=loss_margin)\n",
    "    \n",
    "    @tf.function#(jit_compile=True)\n",
    "    def train_step(self, input_triplets):\n",
    "#         h = 0\n",
    "#         for l in np.ravel(input_triplets.numpy()):\n",
    "#             h += hash(l)\n",
    "#         tf.print(h)\n",
    "        \n",
    "    \n",
    "#         global lastSample, lastModel\n",
    "#         lastSample.assign(tf.add(input_triplets, zero64))\n",
    "#         for i, layer in zip(self.siameseBase.trainable_weights, lastModel):\n",
    "#             layer.assign(tf.add(i, zero32)) \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # training and calculating the error function gradient\n",
    "            representations = self.siameseBase(input_triplets, training=True)\n",
    "            representations = self.normalize_output(representations)\n",
    "            alike = tf.strided_slice(representations, [0,0], tf.shape(representations), strides=[3,1])\n",
    "            anchor = tf.strided_slice(representations, [1,0], tf.shape(representations), strides=[3,1])\n",
    "            unlike = tf.strided_slice(representations, [2,0], tf.shape(representations), strides=[3,1])\n",
    "            loss = self.loss(alike, anchor, unlike)\n",
    "        grads = tape.gradient(loss, self.siameseBase.trainable_weights)\n",
    "\n",
    "        \n",
    "#         alike_in = tf.strided_slice(input_triplets, [0,0,0,0], tf.shape(input_triplets), strides=[3,1,1,1])\n",
    "#         anchor_in = tf.strided_slice(input_triplets, [1,0,0,0], tf.shape(input_triplets), strides=[3,1,1,1])\n",
    "#         unlike_in = tf.strided_slice(input_triplets, [2,0,0,0], tf.shape(input_triplets), strides=[3,1,1,1])\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             alike = self.siameseBase(alike_in, training=True)\n",
    "#         grads1 = tape.gradient(alike, self.siameseBase.trainable_weights) \n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             anchor = self.siameseBase(anchor_in, training=True)\n",
    "#         grads2 = tape.gradient(anchor, self.siameseBase.trainable_weights) \n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             unlike = self.siameseBase(unlike_in, training=True)\n",
    "#         grads3 = tape.gradient(unlike, self.siameseBase.trainable_weights) \n",
    "        \n",
    "#         tf.print(describe(grads1))\n",
    "#         tf.print(describe(alike))\n",
    "#         tf.print(describe(anchor))\n",
    "#         grads1 *= 2*(alike - anchor)\n",
    "#         grads2 *= 2*(unlike - alike)\n",
    "#         grads3 *= 2*(alike - unlike)\n",
    "        \n",
    "#         grads = np.mean([grads1, grads2, grads3])# ...\n",
    "\n",
    "#         loss = self.loss(alike, anchor, unlike)\n",
    "#         h = 0\n",
    "#         for l in tf.experimental.numpy.ravel(self.siameseBase.layers[-1].weights[0]):\n",
    "#             h += hash(l)\n",
    "#         tf.print(\"Przed \", h)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grads, self.siameseBase.trainable_weights))\n",
    "        \n",
    "#         tf.print(self.siameseBase.layers.trainable_weights)\n",
    "#         h = 0\n",
    "#         for l in tf.experimental.numpy.ravel(self.siameseBase.layers[-1].weights[0]):\n",
    "#             h += hash(l)\n",
    "#         tf.print(\"Po \", h)\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "        \n",
    "    def evaluate(self, x=None, y=None, batch_size=None, verbose=False, sample_weight=None, steps=None,\n",
    "                callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "                return_dict=False):\n",
    "        r = self.siameseBase.predict(x=x, batch_size=batch_size, verbose=False, steps=steps, callbacks=callbacks)\n",
    "        r = self.normalize_output(r)\n",
    "        alike = tf.strided_slice(r, [0,0], tf.shape(r), strides=[3,1])\n",
    "        anchor = tf.strided_slice(r, [1,0], tf.shape(r), strides=[3,1])\n",
    "        unlike = tf.strided_slice(r, [2,0], tf.shape(r), strides=[3,1])\n",
    "        dct = {\"loss\":self.loss(alike, anchor, unlike, reduce=tf.reduce_mean)}\n",
    "        if callbacks is not None:\n",
    "            for cb in callbacks:\n",
    "                cb.on_test_batch_end(x, logs=dct)\n",
    "        if return_dict:\n",
    "            return dct\n",
    "        else:\n",
    "            return dct[\"loss\"]\n",
    "\n",
    "\n",
    "#     def __call__(self, x, training=False):\n",
    "#         representations = self.siameseBase(x, training=training)\n",
    "#         representations = self.normalize_output(representations)\n",
    "#         alike = tf.strided_slice(representations, [0,0], tf.shape(representations), strides=[3,1])\n",
    "#         anchor = tf.strided_slice(representations, [1,0], tf.shape(representations), strides=[3,1])\n",
    "#         unlike = tf.strided_slice(representations, [2,0], tf.shape(representations), strides=[3,1])\n",
    "#         return self.loss(alike, anchor, unlike)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.siameseBase.save(path)\n",
    "        \n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.siameseBase.predict(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCA(matrix):\n",
    "    # matrix is 2D prediction data from neural net, first dimension is batch size, second is vector outputs\n",
    "    # thus data is in columns\n",
    "    cov = np.cov(matrix, rowvar=False)\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "    return eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Pipeline initalisation\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a list of paths of dataset images\n",
    "imagesList = tf.io.matching_files(EVAL_DATASET_PATH + \"*.jpg\")\n",
    "DBlen = len(imagesList)\n",
    "\n",
    "# Preparing dataset structure from the list of paths\n",
    "evalpathsDB = tf.data.Dataset.from_tensor_slices(imagesList)\n",
    "\n",
    "# Buidling the dataset pipeline of loading evaluation images for the purpose of loading them all into RAM\n",
    "evalDB = (evalpathsDB.map(tf_read_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "          .batch(32)\n",
    "          .map(tf_convert_and_normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Buidling the dataset pipeline of image classifications\n",
    "evalGrps = (evalpathsDB.map(tf_get_class_from_name, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "            .batch(32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = np.array(list(evalGrps.unbatch().as_numpy_iterator()))\n",
    "\n",
    "np.random.seed(14200)\n",
    "\n",
    "# Reordering the labels using numerical order, rather than alphabetical one\n",
    "labsOrder = np.argsort(labs, kind=\"stable\")\n",
    "labs = labs[labsOrder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the database of samples into numpy array, since it can fit into RAM memory to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesTable = tf_db_to_array(evalDB, DBlen)\n",
    "imagesTable = imagesTable[labsOrder]\n",
    "imagesTable = tf.constant(imagesTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing summary of image classifications for further use\n",
    "By convention, discarding images from group 0, as they have been manually inserted as the examples that differ from the rest sampled from 930k METU dataset.\n",
    "\n",
    "Reordering the labels from alphabetical order into ascending by group number order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the images classifications (groups) summary\n",
    "imageGroupsDct = dict()\n",
    "for i, l in enumerate(labs):\n",
    "    last_count = imageGroupsDct.get(l,(i,0))\n",
    "    imageGroupsDct[l] = (last_count[0], last_count[1] + 1)\n",
    "# Removing the 0-th class, as it's a dummy class of non simliar images\n",
    "del(imageGroupsDct[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into validation and training subsets\n",
    "\n",
    "Taking **validationUniques** samples from each class as uniqiue anchor samples in validation dataset. Pairing those with **xSamples** random samples from original class as similar (alike) sample and any other class as differing (unlike) sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationUniques = 2\n",
    "validationSubset = [(a, [i for i in range(j[0],j[0] + validationUniques)]) for a,j in imageGroupsDct.items()]\n",
    "validationSamples = []\n",
    "xSamples = 40\n",
    "N = len(labs)\n",
    "# Setting the random seed for deterministic choice purposes - to make validation set exactly the same every time\n",
    "\n",
    "for grp, samples in validationSubset:\n",
    "    for sample in samples:\n",
    "        groupStart =  imageGroupsDct[grp][0]\n",
    "        groupLength = imageGroupsDct[grp][1]\n",
    "        for i in range(xSamples):\n",
    "            alike = np.random.randint(groupStart, groupStart + groupLength)\n",
    "            unlike = np.random.randint(0, N-groupLength)\n",
    "            unlike += (unlike>=groupStart)*groupLength\n",
    "            validationSamples.append((alike, sample, unlike))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buliding the translation table\n",
    "\n",
    "The table is necessary as removing some samples from the data set for validation purposes changes the ordering of remaining samples. Instead duplicating data separating those images, it was easier to make the translation on the fly into proper indices from remaining training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTranslationTable = np.array(range(N-validationUniques*len(validationSubset)))\n",
    "starts = [s[0] for s in imageGroupsDct.values()]\n",
    "starts.sort()\n",
    "starts.append(N+1)\n",
    "j = 0\n",
    "for i in range(len(trainingTranslationTable)):\n",
    "    if starts[j] <= i+j*validationUniques:\n",
    "        j += 1\n",
    "    trainingTranslationTable[i] = i+j*validationUniques\n",
    "trainingTranslationTable = tf.constant(trainingTranslationTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating intialisation values for triplet generator\n",
    "\n",
    "Generating the necessary initialisation values such as:\n",
    "\n",
    "    - for each sample, which is to be an anchor in given generator sequence - stored in **avoids** list\n",
    "    - starting indices of each classification group in table of labels/images order (each group is necessary continuous range of indices, as the table is sorted by classification label) - stored in **starts** list\n",
    "    - number of images in said classification group - stored in **lengths** list\n",
    "    - random starting point in fixed sequence - **seeds** list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageGroups = list(imageGroupsDct.items())\n",
    "avoids, starts, lengths, seeds = [], [], [], []\n",
    "\n",
    "# For the sake of being repeatable, fixing seed\n",
    "np.random.seed(949127843)\n",
    "for igrp in imageGroups:\n",
    "    for i in range(igrp[1][1] - validationUniques):\n",
    "        avoids.append(i+igrp[1][0] - (igrp[0]-1)*validationUniques)\n",
    "        starts.append(igrp[1][0] - (igrp[0]-1)*validationUniques)\n",
    "        lengths.append(igrp[1][1] - validationUniques)\n",
    "        seeds.append(np.random.randint(0, high=10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the training data and validation data lengths. \n",
    "**validSamples** defines how many validation images are taken from the data set to make validation subset, while **validLength** defines how many validation triplets are generated from those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLength = len(trainingTranslationTable)\n",
    "validSamples = N - trainLength\n",
    "validLength = len(validationSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the common source of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(41431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data augmentation functions for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training and validation datasets pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_triplet_by_index = triplet_getter(imagesTable)\n",
    "augment = augmenter_getter(rng)\n",
    "translate_indices = translator_getter(trainingTranslationTable)\n",
    "\n",
    "trainDset = tf.data.Dataset.from_generator(triplet_generator,\n",
    "                                    args = [starts, lengths, avoids, trainLength, seeds],\n",
    "                                    output_signature=tf.TensorSpec(shape=(3), dtype=tf.int32))\n",
    "\n",
    "trainDset = (trainDset.shuffle(trainLength, seed=6849)\n",
    "      .map(translate_indices, num_parallel_calls=AUTOTUNE, deterministic=True)\n",
    "      .map(get_triplet_by_index)\n",
    "      .unbatch()\n",
    "# unbatching to augment images individually, as its difficult to make a parallel function for it\n",
    "# TODO: possible improvement\n",
    "      .map(get_center_slice, num_parallel_calls=AUTOTUNE, deterministic=True) # augment\n",
    "      .batch(3*BATCH_SIZE)\n",
    "      .prefetch(2)#AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting the validation dataset samples to fit neatly into batch sizes\n",
    "validDset = (tf.data.Dataset.from_tensor_slices(validationSamples[:(len(validationSamples)//TRUE_BATCH_SIZE)*TRUE_BATCH_SIZE])\n",
    "                .repeat()\n",
    "                .map(get_triplet_by_index)\n",
    "                .unbatch()\n",
    "                .map(get_center_slice) # disabling augmentation\n",
    "                .batch(3*BATCH_SIZE)\n",
    "                .prefetch(2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = trainDset.take(18)\n",
    "# f = iter(f)\n",
    "# f = [next(f) for i in range(18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = 0\n",
    "# for ff in f:\n",
    "#     for l in np.ravel(ff.numpy()):\n",
    "#         h += hash(l)\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, sub = plt.subplots(4,6)\n",
    "# for i in range(4):\n",
    "#     for j in range(6):\n",
    "#         sub[i][j].imshow(f[0][(i*4+j)])\n",
    "#         sub[i][j].axes.get_xaxis().set_visible(False)\n",
    "#         sub[i][j].axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# siameseBase.layers[-1].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Model instantiation and training\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train\n",
    "trainEpochs = 120\n",
    "validationInterval = 5\n",
    "trainEpochs = trainEpochs // validationInterval\n",
    "previewInterval = 6*validationInterval\n",
    "modelSaveInterval = 2*previewInterval\n",
    "\n",
    "enable_validation = {\n",
    "    \"validation_data\":validDset,\n",
    "    \"validation_steps\":validLength//(TRUE_BATCH_SIZE), \n",
    "}\n",
    "disable_validation = {\n",
    "    \"validation_data\":None,\n",
    "    \"validation_steps\":None, \n",
    "    \"validation_batch_size\":None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing/initialising base Siamese model\n",
    "\n",
    "Importing the  pre-trained VGG16 model with weights from imagenet without classification part. Adding 2 dense layers on top of convolutions for 4096 representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "# LOAD = True\n",
    "# LAST_MODEL = \"/home/zenfur/magister/jupyter/model_checkpoints/baseline_20210402-023106_120\"\n",
    "if LOAD:\n",
    "    siameseBase = tf.keras.models.load_model(LAST_MODEL)\n",
    "else:\n",
    "    vgg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3),\n",
    "#     pooling=None,\n",
    "    )\n",
    "    vgg16.trainable = False\n",
    "    siameseBase = tf.keras.models.Sequential()\n",
    "    for layer in (vgg16,\n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(2, activation='sigmoid'),\n",
    "                    #tf.keras.layers.Dense(4096, activation='relu')\n",
    "                 ):\n",
    "        siameseBase.add(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the triplet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripletModel = TripletSiamese(siameseBase, lambda x:x, name=EXPERIMENT_NAME)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "tripletModel.compile(opt, triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorBoardCallback = MeanTBCallback(name=EXPERIMENT_NAME,\n",
    "                                 previewInterval=previewInterval,\n",
    "                                 drawSamplesList=[y for x in [i[1] for i in validationSubset] for y in x],\n",
    "                                 log_dir=LOG_DIR + EXPERIMENT_NAME,\n",
    "                                 histogram_freq=validationInterval,\n",
    "                                 profile_batch=0)\n",
    "\n",
    "\n",
    "# tf.debugging.enable_check_numerics(\n",
    "#     stack_height_limit=300, path_length_limit=500\n",
    "# )\n",
    "\n",
    "checkpointCallback = ModelSaverCallback(modelSaveInterval)\n",
    "# monitor=\"loss\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing training session\n",
    "\n",
    "Before running be sure to check:\n",
    "\n",
    "    - EXPERIMENT_NAME is set to distinct this run from the others\n",
    "    - tensorBoardCallback uses proper log_dir\n",
    "    - trainEpochs, validationInterval, previewInterval, modelSaveInterval are properly set\n",
    "    - network architecture is loaded/initiated as intended\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HIDE_WARNINGS == True:\n",
    "#     logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "    \n",
    "# trainEpochs = 1\n",
    "# try:\n",
    "#     for i in range(lastEpoch, lastEpoch+trainEpochs):\n",
    "#         tripletModel.fit(trainDset, \n",
    "#                           initial_epoch=lastEpoch,\n",
    "#                           epochs=lastEpoch+1,#lastEpoch+validationInterval-1,\n",
    "#                           steps_per_epoch=trainLength//(TRUE_BATCH_SIZE),\n",
    "#                           #callbacks=[tensorBoardCallback, checkpointCallback],\n",
    "#                           **disable_validation\n",
    "#                          ) # batch_size unspecified since the batches are generated by generator\n",
    "#         lastEpoch += 1#validationInterval-1\n",
    "        \n",
    "# #         tripletModel.fit(trainDset, \n",
    "# #                           initial_epoch=lastEpoch,\n",
    "# #                           epochs=lastEpoch+1,\n",
    "# #                           steps_per_epoch=trainLength//(TRUE_BATCH_SIZE),\n",
    "# #                           callbacks=[tensorBoardCallback, checkpointCallback],\n",
    "# #                           **enable_validation\n",
    "# #                          )\n",
    "# #         lastEpoch += 1\n",
    "\n",
    "                \n",
    "# except KeyboardInterrupt as e:\n",
    "#     print(\"Interrupted\")\n",
    "# logging.getLogger('tensorflow').setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.disable_check_numerics()\n",
    "# raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.ravel(siameseBase.layers[-1].weights[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     example = trainDset.take(1)\n",
    "#     example = next(iter(example))\n",
    "#     tripletModel.train_step(example)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "#         {\n",
    "#         \"name\":\"baseline\", # overriden if load path is specified, default == \"\"\n",
    "#         \"loadPath\":\"\", # default == \"\"\n",
    "#         \"loss\":\"triplet_loss\", # default == \"triplet_loss\"\n",
    "#         \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "#         \"vggfreeze\":True,\n",
    "#         \"batchSize\":BATCH_SIZE,\n",
    "#         \"lossParams\":{\"margin\":1.0},\n",
    "#         \"trainFor\":360,\n",
    "#         \"random\":False,\n",
    "#         \"optimiser\":\"adam\",\n",
    "#         \"optimiserParams\":{\"learning_rate\":0.01},\n",
    "#         \"outputNormalisation\":\"max\",\n",
    "#         \"additonalCallbacks\":[], # if specified, get callback functions by name and their params\n",
    "#         \"callbacksArguments\":[]\n",
    "#     },\n",
    "#         {\n",
    "#         \"name\":\"baselineBatchMax\", # overriden if load path is specified, default == \"\"\n",
    "#         \"loadPath\":\"\", # default == \"\"\n",
    "#         \"loss\":\"triplet_loss\", # default == \"triplet_loss\"\n",
    "#         \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "#         \"vggfreeze\":True,\n",
    "#         \"batchSize\":BATCH_SIZE,\n",
    "#         \"lossParams\":{\"margin\":1.0},\n",
    "#         \"trainFor\":360,\n",
    "#         \"random\":False,\n",
    "#         \"optimiser\":\"adam\",\n",
    "#         \"optimiserParams\":{\"learning_rate\":0.01},\n",
    "#         \"outputNormalisation\":\"batch\",\n",
    "#         \"additonalCallbacks\":[], # if specified, get callback functions by name and their params\n",
    "#         \"callbacksArguments\":[]\n",
    "#     },\n",
    "    \n",
    "#         {\n",
    "#         \"name\":\"baselineNorm\", # overriden if load path is specified, default == \"\"\n",
    "#         \"loadPath\":\"\", # default == \"\"\n",
    "#         \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "#         \"trainFor\":360,\n",
    "#         \"outputNormalisation\":\"norm\",\n",
    "#     },    \n",
    "#         {\n",
    "#         \"name\":\"lastLayer2048\", # overriden if load path is specified, default == \"\"\n",
    "#         \"loadPath\":\"\", # default == \"\"\n",
    "#         \"architecture\":[(4096, \"relu\"), (2048, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "#         \"trainFor\":360,\n",
    "#         \"outputNormalisation\":\"max\",\n",
    "#     },    \n",
    "        {\n",
    "        \"name\":\"singleLayer2048\", # overriden if load path is specified, default == \"\"\n",
    "        \"loadPath\":\"\", # default == \"\"\n",
    "        \"architecture\":[(2048, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "        \"trainFor\":360,\n",
    "        \"outputNormalisation\":\"max\",\n",
    "    },\n",
    "#         {\n",
    "#         \"name\":\"singleLayer2048Sigmoid\", # overriden if load path is specified, default == \"\"\n",
    "#         \"loadPath\":\"\", # default == \"\"\n",
    "#         \"architecture\":[(2048, \"sigmoid\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "#         \"trainFor\":360,\n",
    "#         \"outputNormalisation\":None,\n",
    "#     },\n",
    "        {\n",
    "        \"name\":\"baseline\", # overriden if load path is specified, default == \"\"\n",
    "        \"loadPath\":\"\", # default == \"\"\n",
    "        \"loss\":\"triplet_loss\", # default == \"triplet_loss\"\n",
    "        \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "        \"vggfreeze\":True,\n",
    "        \"batchSize\":BATCH_SIZE,\n",
    "        \"lossParams\":{\"margin\":1.0},\n",
    "        \"trainFor\":360,\n",
    "        \"random\":False,\n",
    "        \"optimiser\":\"adam\",\n",
    "        \"optimiserParams\":{\"learning_rate\":0.01},\n",
    "        \"additonalCallbacks\":[], # if specified, get callback functions by name and their params\n",
    "        \"callbacksArguments\":[]\n",
    "    },\n",
    "    {\n",
    "        \"name\":\"baseline\", # overriden if load path is specified, default == \"\"\n",
    "        \"loadPath\":\"\", # default == \"\"\n",
    "        \"loss\":\"triplet_loss\", # default == \"triplet_loss\"\n",
    "        \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "        \"vggfreeze\":True,\n",
    "        \"batchSize\":4,\n",
    "        \"lossParams\":{\"margin\":1.0},\n",
    "        \"trainFor\":960,\n",
    "        \"random\":False,\n",
    "        \"optimiser\":\"adam\",\n",
    "        \"optimiserParams\":{\"learning_rate\":0.01},\n",
    "        \"additonalCallbacks\":[], # if specified, get callback functions by name and their params\n",
    "        \"callbacksArguments\":[]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentDefaults =     {\n",
    "        \"name\":\"baseline\", # overriden if load path is specified, default == \"\"\n",
    "        \"loadPath\":\"\", # default == \"\"\n",
    "        \"loss\":\"triplet_loss\", # default == \"triplet_loss\"\n",
    "        \"architecture\":[(4096, \"relu\"), (4096, \"relu\")], # specify if not loading existing model - list of pairs N, activation function\n",
    "        \"vggfreeze\":True,\n",
    "        \"batchSize\":4,\n",
    "        \"lossParams\":{\"margin\":1.0},\n",
    "        \"trainFor\":360,\n",
    "        \"validationInterval\":5,\n",
    "        \"random\":False,\n",
    "        \"optimiser\":\"adam\",\n",
    "        \"optimiserParams\":{\"learning_rate\":0.01},\n",
    "        \"outputNormalisation\":\"max\",\n",
    "        \"additonalCallbacks\":None, # if specified, get callback functions by name and their params\n",
    "        \"callbacksArguments\":None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    for key, value in experimentDefaults.items():\n",
    "        if key not in experiment:\n",
    "            experiment[key] = value # modifying the original\n",
    "    try:\n",
    "        # Parse load model if path specified\n",
    "        if \"loadPath\" in experiment and experiment[\"loadPath\"]:\n",
    "            baseName = os.path.basename(experiment[\"loadPath\"])\n",
    "            s = baseName.split('_')\n",
    "            # read last epoch from model name\n",
    "            lastEpoch = int(s[-1]) + 1\n",
    "            # set previous experiment name\n",
    "            EXPERIMENT_NAME = \"_\".join(s[:-1])\n",
    "            del s\n",
    "            siameseBase = tf.keras.models.load_model(experiment[\"loadPath\"])\n",
    "        else:\n",
    "        # Model path not specified - has to be initialised\n",
    "            set_experiment_name(experiment[\"name\"])\n",
    "            vgg16 = tf.keras.applications.VGG16(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(224,224,3),\n",
    "            )\n",
    "            lastEpoch=1\n",
    "            \n",
    "            if experiment[\"random\"] == False:\n",
    "                tf.random.set_seed(11123)\n",
    "            else:\n",
    "                tf.random.set_seed(int((datetime.datetime.now()- datetime.datetime(1970,1,1)).total_seconds()))\n",
    "            \n",
    "            # Instantiate the model\n",
    "            vgg16.trainable = not experiment.get(\"vggfreeze\", True)\n",
    "            siameseBase = tf.keras.models.Sequential()\n",
    "            layers = [vgg16, tf.keras.layers.Flatten()]\n",
    "            for layerArch in experiment[\"architecture\"]:\n",
    "                layers.append(tf.keras.layers.Dense(layerArch[0], layerArch[1]))\n",
    "            for layer in layers:\n",
    "                siameseBase.add(layer)\n",
    "        \n",
    "        # Compile the model\n",
    "        if not experiment[\"outputNormalisation\"]:\n",
    "            normalisation_fn = lambda x:x\n",
    "        elif experiment[\"outputNormalisation\"] == \"max\":\n",
    "            normalisation_fn = normalize_output_with_max\n",
    "        elif experiment[\"outputNormalisation\"] == \"batch\":\n",
    "            normalisation_fn = normalize_output_with_max_batch\n",
    "        elif experiment[\"outputNormalisation\"] == \"norm\":\n",
    "            normalisation_fn = normalize_output_with_norm\n",
    "\n",
    "        tripletModel = TripletSiamese(siameseBase, normalisation_fn, name=EXPERIMENT_NAME)\n",
    "        # TODO - choose optimizer according to settings\n",
    "        opt = tf.keras.optimizers.Adam(**experiment[\"optimiserParams\"])\n",
    "        # TODO - choose loss according to  settings\n",
    "        # TODO - use loss params\n",
    "        tripletModel.compile(opt, triplet_loss)\n",
    "\n",
    "        \n",
    "        # (Re)Initialise dataset\n",
    "        batchSize = experiment[\"batchSize\"] or BATCH_SIZE\n",
    "        # TODO - roll generator to match its current epoch if loading\n",
    "        rng = tf.random.Generator.from_seed(41431)\n",
    "        get_triplet_by_index = triplet_getter(imagesTable)\n",
    "        augment = augmenter_getter(rng)\n",
    "        translate_indices = translator_getter(trainingTranslationTable)\n",
    "\n",
    "        newseeds = advance_triplet_generator((lastEpoch-1)*trainLength, lengths, trainLength, seeds)\n",
    "        trainDset = tf.data.Dataset.from_generator(triplet_generator,\n",
    "                                            args = [starts, lengths, avoids, trainLength, newseeds],\n",
    "                                            output_signature=tf.TensorSpec(shape=(3), dtype=tf.int32))\n",
    "\n",
    "        trainDset = (trainDset.shuffle(trainLength)\n",
    "              .map(translate_indices, num_parallel_calls=AUTOTUNE, deterministic=True)\n",
    "              .map(get_triplet_by_index)\n",
    "              .unbatch()\n",
    "              .map(augment, num_parallel_calls=AUTOTUNE, deterministic=True)\n",
    "              .batch(3*batchSize)\n",
    "              .prefetch(2)#AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        tensorBoardCallback = MeanTBCallback(name=EXPERIMENT_NAME,\n",
    "                                 previewInterval=previewInterval,\n",
    "                                 drawSamplesList=[y for x in [i[1] for i in validationSubset] for y in x],\n",
    "                                 log_dir=LOG_DIR + EXPERIMENT_NAME,\n",
    "                                 histogram_freq=validationInterval,\n",
    "                                 profile_batch=0)\n",
    "\n",
    "        checkpointCallback = ModelSaverCallback(modelSaveInterval)\n",
    "        \n",
    "        if HIDE_WARNINGS == True:\n",
    "            logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "        \n",
    "        trainEpochs = experiment[\"trainFor\"]\n",
    "        validationInterval = experiment[\"validationInterval\"]\n",
    "        trainEpochs = (trainEpochs+validationInterval-1) // validationInterval\n",
    "        for i in range(lastEpoch, lastEpoch+trainEpochs):\n",
    "            tripletModel.fit(trainDset, \n",
    "                              initial_epoch=lastEpoch,\n",
    "                              epochs=lastEpoch+validationInterval-1,\n",
    "                              steps_per_epoch=trainLength//(batchSize*3),\n",
    "                              callbacks=[tensorBoardCallback, checkpointCallback],\n",
    "                              **disable_validation\n",
    "                             )\n",
    "            lastEpoch += validationInterval-1\n",
    "\n",
    "            tripletModel.fit(trainDset, \n",
    "                              initial_epoch=lastEpoch,\n",
    "                              epochs=lastEpoch+1,\n",
    "                              steps_per_epoch=trainLength//(batchSize*3),\n",
    "                              callbacks=[tensorBoardCallback, checkpointCallback],\n",
    "                              **enable_validation\n",
    "                             )\n",
    "            lastEpoch += 1\n",
    "\n",
    "#     except KeyboardInterrupt as e:\n",
    "#         print(\"Interrupted\")\n",
    "#     logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    \n",
    "    except Exception as e:\n",
    "        tf.print(f\"Skipping experiment {EXPERIMENT_NAME} due to exception \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Post training analysis\n",
    "---\n",
    "---\n",
    "\n",
    "### Tensorboard plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all last saved models calculating the analysis plots like PCA if they are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(MODEL_DIR)\n",
    "modelsDict = {}\n",
    "# keep only last model from each model sequence\n",
    "for model in models:\n",
    "    splits = model.split('_')\n",
    "    try:\n",
    "        epoch = int(splits[-1])\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    baseName = \"_\".join(splits[:-1])\n",
    "    if modelsDict.get(baseName, 0) < epoch:\n",
    "        modelsDict[baseName] = epoch\n",
    "        \n",
    "modelsToAnalyse = [f\"{MODEL_DIR}{k}_{v}\" for k,v in modelsDict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "for modelName in progressBar(modelsToAnalyse):\n",
    "    tf.keras.backend.clear_session()\n",
    "    name = modelName.split('/')[-1]\n",
    "    pcaPlotName = f\"{ANALYSIS_DIR}{name}_pca.png\"\n",
    "    if not os.path.exists(pcaPlotName):\n",
    "        model = tf.keras.models.load_model(modelName)\n",
    "        rep = model.predict(tf.image.central_crop(imagesTable, 224/256))\n",
    "        pcaVals, pcaVec = getPCA(abs(rep))\n",
    "        f, p = plt.subplots(1,1)\n",
    "        p.semilogy(np.sort(abs(pcaVals))[::-1])\n",
    "        p.set_title(f\"PCA values output of {name}\")\n",
    "        f.savefig(fname=pcaPlotName)\n",
    "    del model\n",
    "    \n",
    "# enable warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(tf.constant([[0,0,0,0], [0,0,0,0]], dtype=tf.dtypes.float32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = tf.constant([[0,0,0,0], [1.0,0,0,0]], dtype=tf.dtypes.float32)\n",
    "x2 = tf.constant([[0,0,0,0], [0,0,0,0]], dtype=tf.dtypes.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    #l = tf.norm(x2-x1, axis=1)\n",
    "    l = tf.reduce_sum(tf.math.squared_difference(x2, x1), axis=1)\n",
    "    #b[b==0.0] = tf.constant(float(\"inf\"))\n",
    "print(tape.gradient(l, x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b == 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[b==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
