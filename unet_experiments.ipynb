{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytywanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MASKS_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/masks/'\n",
    "INPUT_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/jpg/'\n",
    "\n",
    "classes = [o for o in os.listdir(INPUT_PATH) if os.path.isdir(INPUT_PATH + '/' + o)]\n",
    "classes = [o for o in classes if o != 'no-logo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict()\n",
    "targets = dict()\n",
    "queries = dict()\n",
    "start_time = time.time()\n",
    "\n",
    "def rescale(nparray, scale=255.0):\n",
    "    return np.array(nparray, dtype=np.float32)/scale\n",
    "\n",
    "for c in classes:\n",
    "    root_input = INPUT_PATH + '/' + c \n",
    "    root_masks = MASKS_PATH + '/' + c\n",
    "    images[c] = list()\n",
    "    targets[c] = list()\n",
    "    queries[c] = list()\n",
    "    \n",
    "    for f in os.listdir(root_input):\n",
    "        img = cv2.imread(f'{root_input}/{f}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f'{root_masks}/{f}.mask.merged.png', cv2.IMREAD_GRAYSCALE)\n",
    "        bboxes = []\n",
    "        \n",
    "        with open(f'{root_masks}/{f}.bboxes.txt') as csvfile:\n",
    "            bboxread = csv.reader(csvfile, delimiter=' ')\n",
    "            next(bboxread)\n",
    "            for row in bboxread:\n",
    "                bboxes.append(row)\n",
    "                \n",
    "        for bbox in bboxes:\n",
    "            x,y,w,h = [int(i) for i in bbox]\n",
    "            imgslice = img[y:y+h, x:x+w]\n",
    "            imgslice = cv2.resize(imgslice, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "            queries[c].append(rescale(imgslice, 255.0))\n",
    "            # Biore tylko pierwszy z dostepnych bbox na obrazku\n",
    "            break \n",
    "            \n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        mask = cv2.resize(mask, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "        images[c].append(rescale(img, 255.0))\n",
    "        targets[c].append(rescale(mask, 255.0))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(13371337)\n",
    "\n",
    "nclasses = len(classes)\n",
    "nlogos = sum([len(images[c]) for c in classes])//nclasses\n",
    "all_cases = nclasses*nlogos*(nlogos-1)\n",
    "valid_cases = ((all_cases//batch_size)//10)*batch_size\n",
    "valid_unique_n = 2\n",
    "valid_unique_pairs = nclasses*2*np.sum(range(nlogos-1, nlogos-valid_unique_n-1, -1))\n",
    "\n",
    "train_data_permutations = np.zeros((all_cases-valid_cases, 3), dtype=np.int8)\n",
    "valid_data_permutations = np.zeros((valid_cases, 3), dtype=np.int8)\n",
    "\n",
    "skips = np.sort(rnd.choice(all_cases-valid_cases, size=valid_cases-valid_unique_pairs+1, replace=False))\n",
    "skips[-1] = all_cases\n",
    "\n",
    "trainIt = 0\n",
    "validIt = 0\n",
    "skipIt = 0\n",
    "for c_i in range(nclasses):\n",
    "    valid_unique = rnd.choice(nlogos, size=valid_unique_n, replace=False)\n",
    "    for n_i in range(nlogos):\n",
    "        for l_i in range(nlogos):\n",
    "            if n_i == l_i:\n",
    "                continue\n",
    "            if n_i in valid_unique or l_i in valid_unique:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "            elif skips[skipIt] == trainIt:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "                skipIt += 1\n",
    "            else:\n",
    "                train_data_permutations[trainIt] = (c_i, n_i, l_i)                \n",
    "                trainIt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_permutations = rnd.permutation(train_data_permutations)\n",
    "train_data_permutations = train_data_permutations[:(len(train_data_permutations)//5//batch_size)*batch_size]\n",
    "valid_data_permutations = rnd.permutation(valid_data_permutations)\n",
    "valid_data_permutations = valid_data_permutations[:(len(valid_data_permutations)//4//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3)),\n",
    "                       np.reshape(outquery, (batch_size, 64, 64, 3))\n",
    "                      ), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator_dummy(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3))), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetValidDataset = tf.data.Dataset.from_generator(dataset_permutations_generator_dummy,\n",
    "                                             args=[batch_size, valid_data_permutations],\n",
    "                                             output_types=((tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3)),# (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetTrainDataset = tf.data.Dataset.from_generator(dataset_permutations_generator_dummy,\n",
    "                                             args=[batch_size, train_data_permutations],\n",
    "                                             output_types=((tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3)),# (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Stałe i parametry warstw splotowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightDecay = 0.0005\n",
    "momentum = 0.9\n",
    "learningRate = 0.04\n",
    "L2penalty = weightDecay\n",
    "experimentName = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convOptions = {\n",
    "    \"strides\": 1,\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "\n",
    "convTransOptions = {\n",
    "    \"strides\": (2,2),\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "maxPoolOptions = {\n",
    "    \"pool_size\": 2,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": 'SAME'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część transenkodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None, 256, 256, None, 512, 512, None, 512, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część conditional branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputQuery  = tf.keras.Input(dtype = tf.float32, shape = [64, 64, 3], name = 'Query')\n",
    "layersConditionalEncoder = [LInputQuery]\n",
    "with tf.name_scope(\"Conditional\"):\n",
    "    filtersNumber=[32, 32, None, 64, 64, None, 128, None, 256, None, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "        else:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = 512, \n",
    "                                       kernel_size=2, \n",
    "                                       strides=2, \n",
    "                                       **{k:v for k,v in convOptions.items() if k != 'strides'}\n",
    "                                      )(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    conditionalEncoderOutput = layersConditionalEncoder[-1]\n",
    "\n",
    "    modelConditional = tf.keras.Model(\n",
    "        inputs=LInputQuery,\n",
    "        outputs=conditionalEncoderOutput, \n",
    "        name=\"Latent Representation Encoder\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część transdekodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    tiles = [(8,8), (16,16), (32, 32), (64, 64), (128,128)]\n",
    "    filters = [(None, 512, 512, 512), \n",
    "               (512, 512, 512, 512), \n",
    "               (256, 256, 256, 256), \n",
    "               (128, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for tile, encodedInput, fs in zip(tiles, reversed(transcoderInputs), filters):\n",
    "        # Tiling output from conditional encoder\n",
    "        #layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=tile)(conditionalEncoderOutput))\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnet = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom gradients logging callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientsLoggerTBCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, gradient_reference, *args, **kwargs):\n",
    "        super(GradientsLoggerTBCallback, self).__init__(*args, **kwargs)\n",
    "        self._gradient_ref = gradient_reference\n",
    "        self._epoch = 1\n",
    "        self.once = True\n",
    "        \n",
    "    def _get_gradient(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(self._gradient_ref[0], training=True)  # Forward pass\n",
    "            loss = self.model.compiled_loss(y_true=self._gradient_ref[1], y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        return gradients\n",
    "\n",
    "    def _log_gradients(self, epoch):\n",
    "        writer = self._get_writer(self._train_run_name)\n",
    "        gradients = self._get_gradient()\n",
    "\n",
    "        with writer.as_default():\n",
    "            # Getting names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "        writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(GradientsLoggerTBCallback, self).on_epoch_end(epoch, logs=logs)\n",
    "        \n",
    "        self._epoch += 1\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)\n",
    "    \n",
    "    #def on_train_batch_end(self, batch, logs=None):\n",
    "        #if self.histogram_freq and self._epoch % self.histogram_freq == 0 and self.once:\n",
    "            #print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "            #print(self.__dir__())\n",
    "            #print(logs)\n",
    "            #self.once = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp1_wo_cond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompilacja modelu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnet.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                          label_smoothing=0, \n",
    "                                                          reduction=\"auto\", \n",
    "                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy_metric\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "                                                                   #profile_batch = '1,3')\n",
    "except AlreadyExistsError:\n",
    "    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.fit(unetTrainDataset, \n",
    "              epochs=7, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=valid_cases//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp2_wo_cond_2layer\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    filters = [(None, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for encodedInput, fs in zip(reversed(transcoderInputs), filters):\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnetEx2 = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnetEx2.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                          label_smoothing=0, \n",
    "                                                          reduction=\"auto\", \n",
    "                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy_metric\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "except AlreadyExistsError:\n",
    "    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnetEx2.fit(unetTrainDataset, \n",
    "              epochs=3, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=valid_cases//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp3_wo_cond_1layer\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    filters = [(None, 64, 64, 64)]\n",
    "    \n",
    "    for encodedInput, fs in zip(reversed(transcoderInputs), filters):\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        #layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=(2,2))(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnetEx3 = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnetEx3.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                          label_smoothing=0, \n",
    "                                                          reduction=\"auto\", \n",
    "                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy_metric\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "finally:\n",
    "    pass\n",
    "#except AlreadyExistsError:\n",
    "#    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnetEx3.fit(unetTrainDataset, \n",
    "              epochs=10, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=valid_cases//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
