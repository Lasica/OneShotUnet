{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No compatible GPUs found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightDecay = 0.0\n",
    "momentum = 0.9\n",
    "learningRate = 0.0004\n",
    "batch_size = 16\n",
    "L2penalty = weightDecay/learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convOptions = {\n",
    "    \"strides\": 1,\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    \"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"use_bias\": False, \n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "\n",
    "convTransOptions = {\n",
    "    \"strides\": (2,2),\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    \"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "maxPoolOptions = {\n",
    "    \"pool_size\": 2,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": 'SAME'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC specifics, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/masks/'\n",
    "INPUT_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/jpg/'\n",
    "modelSaveName = \"batch_normalized\"#None # Edit to save the model after training\n",
    "LOGS_PATH = \"/qarr/studia/magister/models/logs/\" + modelSaveName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_CHECKPOINT_PATH = \"/qarr/studia/magister/models/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model architecture\n",
    "\n",
    "Firstly, clear the session to remove any lingering variables/models in memory from eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the first part of transcoder - the encoder reducing the dimensions of the input in VGG manner using 2 CNN steps with 3x3 kernels followed with the MaxPool layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None, 256, 256, None, 512, 512, None, 512, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.BatchNormalization()(layersEncoder[-1]))\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelEncoder, \"unet_encoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the conditional branch that translates input querry in VGG-like stack to 1x1x512 dimensions to tile and then corelate with consequent steps of the decoder part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputQuery  = tf.keras.Input(dtype = tf.float32, shape = [64, 64, 3], name = 'Query')\n",
    "layersConditionalEncoder = [LInputQuery]\n",
    "with tf.name_scope(\"Conditional\"):\n",
    "    filtersNumber=[32, 32, None, 64, 64, None, 128, None, 256, None, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersConditionalEncoder.append(tf.keras.layers.BatchNormalization()(layersConditionalEncoder[-1]))\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "        else:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    # Todo replace with fully connected x2\n",
    "    layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = 512, \n",
    "                                       kernel_size=2, \n",
    "                                       strides=2, \n",
    "                                       **{k:v for k,v in convOptions.items() if k != 'strides'}\n",
    "                                      )(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    #layersConditionalEncoder.append(tf.keras.layers.BatchNormalization()(layersConditionalEncoder[-1]))\n",
    "    conditionalEncoderOutput = layersConditionalEncoder[-1]\n",
    "\n",
    "    modelConditional = tf.keras.Model(\n",
    "        inputs=LInputQuery,\n",
    "        outputs=conditionalEncoderOutput, \n",
    "        name=\"Latent Representation Encoder\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConditional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelConditional, \"unet_encoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the decoder branch that combines tiled conditonal branch results with the consequent steps of the encoder part to detect the pattern of conditionally trained branch on different scopes of encoded resolution.\n",
    "\n",
    "Defining custom Softmax layer, because current 2.3.1 tf implementation has a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import tf_utils\n",
    "class Softmax(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=-1, **kwargs):\n",
    "    super(Softmax, self).__init__(**kwargs)\n",
    "    self.supports_masking = True\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.keras.activations.softmax(inputs, axis=self.axis)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {'axis': self.axis}\n",
    "    base_config = super(Softmax, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  @tf_utils.shape_type_conversion\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    tiles = [(8,8), (16,16), (32, 32), (64, 64), (128,128)]\n",
    "    filters = [(None, 512, 512, 512), \n",
    "               (512, 512, 512, 512), \n",
    "               (256, 256, 256, 256), \n",
    "               (128, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for tile, encodedInput, fs in zip(tiles, reversed(transcoderInputs), filters):\n",
    "        # Tiling output from conditional encoder\n",
    "        layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=tile)(conditionalEncoderOutput))\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], encodedInput]))\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        \n",
    "        layersTransDecoder.append(tf.keras.layers.BatchNormalization()(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.BatchNormalization()(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "    \n",
    "    layersTransDecoder.append(Softmax(axis=[1,2])(layersTransDecoder[-1])) #Experimental\n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    #unetOutput = Softmax(axis=[1,2])(unetOutput)\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnet = tf.keras.Model(inputs=[LInputTarget, LInputQuery], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelUnet, \"unet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model with SGD optimizer and BinaryCrossentropy loss\n",
    "\n",
    "Weight decay to be implemented using L2 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_bce_loss(y_true, y_pred, ratio = 0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    dtype = y_pred.dtype.base_dtype\n",
    "    epsilon = tf.keras.backend.epsilon\n",
    "    shape = y_true.shape\n",
    "    pixels = np.prod(shape)\n",
    "    \n",
    "    mask = tf.greater_equal(y_true, 0.5)\n",
    "    mask = tf.cast(mask, dtype)\n",
    "    whites = tf.math.count_nonzero(mask, dtype=dtype) # sum?\n",
    "    whites_weight = ratio * pixels / (whites + epsilon())\n",
    "    blacks_weight = (1 - ratio) * pixels / (pixels - whites + epsilon())\n",
    "    mask = tf.multiply(mask, whites_weight - blacks_weight)\n",
    "    mask = tf.add(mask, blacks_weight)\n",
    "    \n",
    "    # mean over whole batch\n",
    "    \n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    \n",
    "    #epsilon_ = _constant_to_tensor(epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon(), 1. - epsilon())\n",
    "\n",
    "    # Compute cross entropy from probabilities.\n",
    "    bce = y_true * tf.math.log(y_pred + epsilon())\n",
    "    bce += (1 - y_true) * tf.math.log(1 - y_pred + epsilon())\n",
    "    bce = tf.multiply(bce, mask)\n",
    "    return -bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") # weight decay 0.0005 by L2\n",
    "\n",
    "modelUnet.compile(optimizer=optimizer,\n",
    "              #custom_bce_loss,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              #metrics=[tf.keras.metrics.TruePositives(),\n",
    "              #         tf.keras.metrics.TrueNegatives(),]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [o for o in os.listdir(INPUT_PATH) if os.path.isdir(INPUT_PATH + '/' + o)]\n",
    "classes = [o for o in classes if o != 'no-logo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pictures into numpy arrays, slicing and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict()\n",
    "targets = dict()\n",
    "queries = dict()\n",
    "start_time = time.time()\n",
    "\n",
    "def rescale(nparray, scale=255.0):\n",
    "    return np.array(nparray, dtype=np.float32)/scale\n",
    "\n",
    "for c in classes:\n",
    "    root_input = INPUT_PATH + '/' + c \n",
    "    root_masks = MASKS_PATH + '/' + c\n",
    "    images[c] = list()\n",
    "    targets[c] = list()\n",
    "    queries[c] = list()\n",
    "    \n",
    "    for f in os.listdir(root_input):\n",
    "        img = cv2.imread(f'{root_input}/{f}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f'{root_masks}/{f}.mask.merged.png', cv2.IMREAD_GRAYSCALE)\n",
    "        bboxes = []\n",
    "        \n",
    "        with open(f'{root_masks}/{f}.bboxes.txt') as csvfile:\n",
    "            bboxread = csv.reader(csvfile, delimiter=' ')\n",
    "            next(bboxread)\n",
    "            for row in bboxread:\n",
    "                bboxes.append(row)\n",
    "                \n",
    "        for bbox in bboxes:\n",
    "            x,y,w,h = [int(i) for i in bbox]\n",
    "            imgslice = img[y:y+h, x:x+w]\n",
    "            imgslice = cv2.resize(imgslice, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "            queries[c].append(rescale(imgslice, 255.0))\n",
    "            # Biore tylko pierwszy z dostepnych bbox na obrazku\n",
    "            break \n",
    "            \n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        mask = cv2.resize(mask, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "        images[c].append(rescale(img, 255.0))\n",
    "        targets[c].append(rescale(mask, 255.0))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing some loaded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in images.values():\n",
    "    plt.imshow(v[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in targets.values():\n",
    "    plt.imshow(v[0], cmap='gist_gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in queries.values():\n",
    "    plt.imshow(v[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    print(f'{c:>12}: {len(images[c])} logos: {len(queries[c]):3<} pairs: {len(images[c])*(len(queries[c])-1)}')\n",
    "print(f'{\"total\":<12}: {sum([len(images[c]) for c in classes])} logos: {sum([len(queries[c]) for c in classes])} pairs: {sum([len(images[c])*len(queries[c]) for c in classes])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of size between static dataset generated from triples and dynamicly generated triplets from pairs in memory. Given the size of the triplets it's unfeasible to generate this dataset and save on SDD in order to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{32*(70*69)*(64*64*3+256*256*4)*8/1e9} GB vs {32*(70)*(64*64*3+256*256*4)*8/1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering triplets count in the case of taking all samples from given image, not only one per image:\n",
    "```\n",
    "      adidas: 70 logos: 120 pairs: 8400\n",
    "        aldi: 70 logos: 106 pairs: 7420\n",
    "       apple: 70 logos: 76 pairs: 5320\n",
    "       becks: 70 logos: 100 pairs: 7000\n",
    "         bmw: 70 logos: 74 pairs: 5180\n",
    "   carlsberg: 70 logos: 108 pairs: 7560\n",
    "      chimay: 70 logos: 112 pairs: 7840\n",
    "    cocacola: 70 logos: 130 pairs: 9100\n",
    "      corona: 70 logos: 83 pairs: 5810\n",
    "         dhl: 70 logos: 123 pairs: 8610\n",
    "    erdinger: 70 logos: 105 pairs: 7350\n",
    "        esso: 70 logos: 87 pairs: 6090\n",
    "       fedex: 70 logos: 94 pairs: 6580\n",
    "     ferrari: 70 logos: 73 pairs: 5110\n",
    "        ford: 70 logos: 76 pairs: 5320\n",
    "     fosters: 70 logos: 98 pairs: 6860\n",
    "      google: 70 logos: 83 pairs: 5810\n",
    "     guiness: 70 logos: 98 pairs: 6860\n",
    "    heineken: 70 logos: 103 pairs: 7210\n",
    "          hp: 70 logos: 112 pairs: 7840\n",
    "       milka: 70 logos: 197 pairs: 13790\n",
    "      nvidia: 70 logos: 114 pairs: 7980\n",
    "    paulaner: 70 logos: 102 pairs: 7140\n",
    "       pepsi: 70 logos: 178 pairs: 12460\n",
    " rittersport: 70 logos: 204 pairs: 14280\n",
    "       shell: 70 logos: 96 pairs: 6720\n",
    "      singha: 70 logos: 83 pairs: 5810\n",
    "   starbucks: 70 logos: 95 pairs: 6650\n",
    "stellaartois: 70 logos: 87 pairs: 6090\n",
    "      texaco: 70 logos: 88 pairs: 6160\n",
    "    tsingtao: 70 logos: 109 pairs: 7630\n",
    "         ups: 70 logos: 90 pairs: 6300\n",
    "total       : 2240 logos: 3404 pairs: 238280\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the triplets dataset generator from binary pairs\n",
    "\n",
    "Additional gymnastics to:\n",
    " - fix the random seed to get reproducible results\n",
    " - reserve 2 unique images per class and all pairs within those for validation set,\n",
    " - fill the validation dataset to meet the assumed ratio to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(13371337)\n",
    "\n",
    "nclasses = len(classes)\n",
    "nlogos = sum([len(images[c]) for c in classes])//nclasses\n",
    "all_cases = nclasses*nlogos*(nlogos-1)\n",
    "valid_cases = ((all_cases//batch_size)//10)*batch_size\n",
    "valid_unique_n = 2\n",
    "valid_unique_pairs = nclasses*2*np.sum(range(nlogos-1, nlogos-valid_unique_n-1, -1))\n",
    "\n",
    "train_data_permutations = np.zeros((all_cases-valid_cases, 3), dtype=np.int8)\n",
    "valid_data_permutations = np.zeros((valid_cases, 3), dtype=np.int8)\n",
    "\n",
    "skips = np.sort(rnd.choice(all_cases-valid_cases, size=valid_cases-valid_unique_pairs+1, replace=False))\n",
    "skips[-1] = all_cases\n",
    "\n",
    "trainIt = 0\n",
    "validIt = 0\n",
    "skipIt = 0\n",
    "for c_i in range(nclasses):\n",
    "    valid_unique = rnd.choice(nlogos, size=valid_unique_n, replace=False)\n",
    "    for n_i in range(nlogos):\n",
    "        for l_i in range(nlogos):\n",
    "            if n_i == l_i:\n",
    "                continue\n",
    "            if n_i in valid_unique or l_i in valid_unique:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "            elif skips[skipIt] == trainIt:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "                skipIt += 1\n",
    "            else:\n",
    "                train_data_permutations[trainIt] = (c_i, n_i, l_i)                \n",
    "                trainIt += 1\n",
    "                \n",
    "train_data_permutations = rnd.permutation(train_data_permutations)\n",
    "valid_data_permutations = rnd.permutation(valid_data_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decimating the training set in order to iterate faster with debugging purposes, to be removed in final approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1\n",
    "train_data_permutations = train_data_permutations[:(len(train_data_permutations)//factor//batch_size)*batch_size]\n",
    "valid_data_permutations = valid_data_permutations[:(len(valid_data_permutations)//factor//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the data shape to make sure it is right after the decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    try:\n",
    "        return f'{x.shape}'\n",
    "    except AttributeError:\n",
    "        return f\"{'[' + ', '.join([describe(q) for q in x]) + ']'}\"\n",
    "\n",
    "describe(train_data_permutations)\n",
    "print(all_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the generator fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3)),\n",
    "                       np.reshape(outquery, (batch_size, 64, 64, 3))\n",
    "                      ), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = dataset_permutations_generator(1, train_data_permutations, shuffle=False)\n",
    "tdat = next(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tdat[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the validation and training datasets as tf.data.Dataset using generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been forced to use batch_size of 24 rather than 32 due to limitations of memory on my GPU (Nvidia Geforce GTX1070 with 8GB RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetValidDataset = tf.data.Dataset.from_generator(dataset_permutations_generator,\n",
    "                                             args=[batch_size, valid_data_permutations],\n",
    "                                             output_types=((tf.float32, tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3), (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            ).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetTrainDataset = tf.data.Dataset.from_generator(dataset_permutations_generator,\n",
    "                                             args=[batch_size, train_data_permutations],\n",
    "                                             output_types=((tf.float32, tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3), (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            ).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Creating callbacks for data visualisation and model saving - enable by uncommenting those in fit method call. Defining custom callback to log gradients on selected example batch and view those in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRawGradients = False\n",
    "\n",
    "class GradientsLoggerTBCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, gradient_reference, logRawGradients = True, *args, **kwargs):\n",
    "        super(GradientsLoggerTBCallback, self).__init__(*args, **kwargs)\n",
    "        self._gradient_ref = gradient_reference\n",
    "        self.gradient_logs = []\n",
    "        self.logRawGradients = logRawGradients\n",
    "        self._epoch = 1\n",
    "        self.once = True\n",
    "        \n",
    "    def _get_gradient(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(self._gradient_ref[0], training=True)  # Forward pass\n",
    "            loss = self.model.compiled_loss(y_true=self._gradient_ref[1], y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        return gradients\n",
    "\n",
    "    def _log_gradients(self, epoch):\n",
    "        # changes in version \"2.2.0\"\n",
    "        if tf.version.VERSION.split('.')[1] <= '2':\n",
    "            writer = self._get_writer(self._train_run_name)\n",
    "        else:\n",
    "            writer = self._train_writer\n",
    "        gradients = self._get_gradient()\n",
    "        if self.logRawGradients:\n",
    "            self.gradient_logs.append(gradients)\n",
    "        \n",
    "        with writer.as_default():\n",
    "            # Getting names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "            writer.flush()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        super(GradientsLoggerTBCallback, self).on_epoch_end(epoch, logs=logs)\n",
    "        \n",
    "        self._epoch += 1\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)\n",
    "    \n",
    "    #def on_train_batch_end(self, batch, logs=None):\n",
    "        #if self.histogram_freq and self._epoch % self.histogram_freq == 0 and self.once:\n",
    "            #print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "            #print(self.__dir__())\n",
    "            #print(logs)\n",
    "            #self.once = False\n",
    "\n",
    "            \n",
    "# Create a TensorBoard callback\n",
    "#try:\n",
    "tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                 logRawGradients = logRawGradients,\n",
    "                                                 log_dir = LOGS_PATH,\n",
    "                                                 histogram_freq = 1)\n",
    "                                                 #profile_batch = '1,3')\n",
    "#tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = LOGS_PATH, histogram_freq = 1)\n",
    "#except AlreadyExistsError:\n",
    "#    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    MODEL_CHECKPOINT_PATH,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the fit method\n",
    "\n",
    "Adjust epochs parameter before calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    modelUnet.fit(unetTrainDataset, \n",
    "                  epochs=5, \n",
    "                  steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "                  validation_data=unetValidDataset,\n",
    "                  validation_steps=len(valid_data_permutations)//batch_size,\n",
    "                  callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "                 ) # batch_size unspecified since it's generated by generator\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelSaveName is not None:\n",
    "    modelUnet.save(modelSaveName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing and analysing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unetTrainDataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_result = modelUnet.predict(example)\n",
    "example = list(example.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = plt.subplots(1,4)\n",
    "subs = subs[0].axes\n",
    "subs[0].imshow(example[0][0][1][0])\n",
    "subs[1].imshow(example[0][0][0][0])\n",
    "subs[2].imshow(np.reshape(example[0][1][0], (256,256)), cmap='gist_gray')\n",
    "subs[3].imshow(np.reshape(example_result[0], (256,256)), cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing means and variance of gradient after the first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logRawGradients:\n",
    "    print(\n",
    "        '\\n'.join(\n",
    "            [str(a.shape) + \"): \" + b + \"\\n gradient mean:\" + str(np.mean(a)) + ' variance:' + str(np.var(a))  for a,b in \n",
    "                 zip(tboard_callback.gradient_logs[0], \n",
    "                     [tw.name for tw in modelUnet.trainable_weights])\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelSaveName is not None:\n",
    "    modelUnet = tf.keras.models.load_model(modelSaveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
