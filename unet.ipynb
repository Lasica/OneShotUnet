{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightDecay = 0.0005\n",
    "momentum = 0.9\n",
    "learningRate = 0.4\n",
    "batch_size = 24\n",
    "L2penalty = weightDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "convOptions = {\n",
    "    \"strides\": 1,\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "\n",
    "convTransOptions = {\n",
    "    \"strides\": (2,2),\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "maxPoolOptions = {\n",
    "    \"pool_size\": 2,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": 'SAME'\n",
    "}\n",
    "\n",
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None, 256, 256, None, 512, 512, None, 512, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelEncoder, \"unet_encoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputQuery  = tf.keras.Input(dtype = tf.float32, shape = [64, 64, 3], name = 'Query')\n",
    "layersConditionalEncoder = [LInputQuery]\n",
    "with tf.name_scope(\"Conditional\"):\n",
    "    filtersNumber=[32, 32, None, 64, 64, None, 128, None, 256, None, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "        else:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = 512, \n",
    "                                       kernel_size=2, \n",
    "                                       strides=2, \n",
    "                                       **{k:v for k,v in convOptions.items() if k != 'strides'}\n",
    "                                      )(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    conditionalEncoderOutput = layersConditionalEncoder[-1]\n",
    "\n",
    "    modelConditional = tf.keras.Model(\n",
    "        inputs=LInputQuery,\n",
    "        outputs=conditionalEncoderOutput, \n",
    "        name=\"Latent Representation Encoder\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConditional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelConditional, \"unet_encoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    tiles = [(8,8), (16,16), (32, 32), (64, 64), (128,128)]\n",
    "    filters = [(None, 512, 512, 512), \n",
    "               (512, 512, 512, 512), \n",
    "               (256, 256, 256, 256), \n",
    "               (128, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for tile, encodedInput, fs in zip(tiles, reversed(transcoderInputs), filters):\n",
    "        # Tiling output from conditional encoder\n",
    "        layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=tile)(conditionalEncoderOutput))\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], encodedInput]))\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnet = tf.keras.Model(inputs=[LInputTarget, LInputQuery], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{35659553 * (32/8)/1e6}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(modelUnet, \"unet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MASKS_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/masks/'\n",
    "INPUT_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/jpg/'\n",
    "\n",
    "classes = [o for o in os.listdir(INPUT_PATH) if os.path.isdir(INPUT_PATH + '/' + o)]\n",
    "classes = [o for o in classes if o != 'no-logo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie obrazków do pamięci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict()\n",
    "targets = dict()\n",
    "queries = dict()\n",
    "start_time = time.time()\n",
    "\n",
    "def rescale(nparray, scale=255.0):\n",
    "    return np.array(nparray, dtype=np.float32)/scale\n",
    "\n",
    "for c in classes:\n",
    "    root_input = INPUT_PATH + '/' + c \n",
    "    root_masks = MASKS_PATH + '/' + c\n",
    "    images[c] = list()\n",
    "    targets[c] = list()\n",
    "    queries[c] = list()\n",
    "    \n",
    "    for f in os.listdir(root_input):\n",
    "        img = cv2.imread(f'{root_input}/{f}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f'{root_masks}/{f}.mask.merged.png', cv2.IMREAD_GRAYSCALE)\n",
    "        bboxes = []\n",
    "        \n",
    "        with open(f'{root_masks}/{f}.bboxes.txt') as csvfile:\n",
    "            bboxread = csv.reader(csvfile, delimiter=' ')\n",
    "            next(bboxread)\n",
    "            for row in bboxread:\n",
    "                bboxes.append(row)\n",
    "                \n",
    "        for bbox in bboxes:\n",
    "            x,y,w,h = [int(i) for i in bbox]\n",
    "            imgslice = img[y:y+h, x:x+w]\n",
    "            imgslice = cv2.resize(imgslice, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "            queries[c].append(rescale(imgslice, 255.0))\n",
    "            # Biore tylko pierwszy z dostepnych bbox na obrazku\n",
    "            break \n",
    "            \n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        mask = cv2.resize(mask, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "        images[c].append(rescale(img, 255.0))\n",
    "        targets[c].append(rescale(mask, 255.0))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytane przykłady obrazków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in images.values():\n",
    "    plt.imshow(v[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in targets.values():\n",
    "    plt.imshow(v[0], cmap='gist_gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in queries.values():\n",
    "    plt.imshow(v[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza rozmiaru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    print(f'{c:>12}: {len(images[c])} logos: {len(queries[c]):3<} pairs: {len(images[c])*(len(queries[c])-1)}')\n",
    "print(f'{\"total\":<12}: {sum([len(images[c]) for c in classes])} logos: {sum([len(queries[c]) for c in classes])} pairs: {sum([len(images[c])*len(queries[c]) for c in classes])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{32*(70*69)*(64*64*3+256*256*4)*8/1e9} GB vs {32*(70)*(64*64*3+256*256*4)*8/1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdyby brać pod uwagę każde logo z obrazka:\n",
    "```\n",
    "      adidas: 70 logos: 120 pairs: 8400\n",
    "        aldi: 70 logos: 106 pairs: 7420\n",
    "       apple: 70 logos: 76 pairs: 5320\n",
    "       becks: 70 logos: 100 pairs: 7000\n",
    "         bmw: 70 logos: 74 pairs: 5180\n",
    "   carlsberg: 70 logos: 108 pairs: 7560\n",
    "      chimay: 70 logos: 112 pairs: 7840\n",
    "    cocacola: 70 logos: 130 pairs: 9100\n",
    "      corona: 70 logos: 83 pairs: 5810\n",
    "         dhl: 70 logos: 123 pairs: 8610\n",
    "    erdinger: 70 logos: 105 pairs: 7350\n",
    "        esso: 70 logos: 87 pairs: 6090\n",
    "       fedex: 70 logos: 94 pairs: 6580\n",
    "     ferrari: 70 logos: 73 pairs: 5110\n",
    "        ford: 70 logos: 76 pairs: 5320\n",
    "     fosters: 70 logos: 98 pairs: 6860\n",
    "      google: 70 logos: 83 pairs: 5810\n",
    "     guiness: 70 logos: 98 pairs: 6860\n",
    "    heineken: 70 logos: 103 pairs: 7210\n",
    "          hp: 70 logos: 112 pairs: 7840\n",
    "       milka: 70 logos: 197 pairs: 13790\n",
    "      nvidia: 70 logos: 114 pairs: 7980\n",
    "    paulaner: 70 logos: 102 pairs: 7140\n",
    "       pepsi: 70 logos: 178 pairs: 12460\n",
    " rittersport: 70 logos: 204 pairs: 14280\n",
    "       shell: 70 logos: 96 pairs: 6720\n",
    "      singha: 70 logos: 83 pairs: 5810\n",
    "   starbucks: 70 logos: 95 pairs: 6650\n",
    "stellaartois: 70 logos: 87 pairs: 6090\n",
    "      texaco: 70 logos: 88 pairs: 6160\n",
    "    tsingtao: 70 logos: 109 pairs: 7630\n",
    "         ups: 70 logos: 90 pairs: 6300\n",
    "total       : 2240 logos: 3404 pairs: 238280\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(13371337)\n",
    "\n",
    "nclasses = len(classes)\n",
    "nlogos = sum([len(images[c]) for c in classes])//nclasses\n",
    "all_cases = nclasses*nlogos*(nlogos-1)\n",
    "valid_cases = ((all_cases//batch_size)//10)*batch_size\n",
    "valid_unique_n = 2\n",
    "valid_unique_pairs = nclasses*2*np.sum(range(nlogos-1, nlogos-valid_unique_n-1, -1))\n",
    "\n",
    "train_data_permutations = np.zeros((all_cases-valid_cases, 3), dtype=np.int8)\n",
    "valid_data_permutations = np.zeros((valid_cases, 3), dtype=np.int8)\n",
    "\n",
    "skips = np.sort(rnd.choice(all_cases-valid_cases, size=valid_cases-valid_unique_pairs+1, replace=False))\n",
    "skips[-1] = all_cases\n",
    "\n",
    "trainIt = 0\n",
    "validIt = 0\n",
    "skipIt = 0\n",
    "for c_i in range(nclasses):\n",
    "    valid_unique = rnd.choice(nlogos, size=valid_unique_n, replace=False)\n",
    "    for n_i in range(nlogos):\n",
    "        for l_i in range(nlogos):\n",
    "            if n_i == l_i:\n",
    "                continue\n",
    "            if n_i in valid_unique or l_i in valid_unique:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "            elif skips[skipIt] == trainIt:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "                skipIt += 1\n",
    "            else:\n",
    "                train_data_permutations[trainIt] = (c_i, n_i, l_i)                \n",
    "                trainIt += 1\n",
    "                \n",
    "train_data_permutations = rnd.permutation(train_data_permutations)\n",
    "train_data_permutations = train_data_permutations[:(len(train_data_permutations)//5//batch_size)*batch_size]\n",
    "valid_data_permutations = rnd.permutation(valid_data_permutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    try:\n",
    "        return f'{x.shape}'\n",
    "    except AttributeError:\n",
    "        return f\"{'[' + ', '.join([describe(q) for q in x]) + ']'}\"\n",
    "\n",
    "describe(train_data_permutations)\n",
    "print(all_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3)),\n",
    "                       np.reshape(outquery, (batch_size, 64, 64, 3))\n",
    "                      ), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = dataset_permutations_generator(1, train_data_permutations, shuffle=False)\n",
    "tdat = next(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tdat[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "unetDataset = tf.data.Dataset.from_generator(dataset_permutations_generator,\n",
    "                                             args=[batch_size, train_data_permutations],\n",
    "                                             output_types=((tf.float32, tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3), (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )\n",
    "#unetDataset = unetDataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zestawienie modelu i uczenie - próba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") # weight decay 0.0005 by L2\n",
    "\n",
    "modelUnet.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryCrossentropy(\n",
    "                        name=\"binary_crossentropy\")]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próba generalna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetValidDataset = tf.data.Dataset.from_generator(dataset_permutations_generator,\n",
    "                                             args=[batch_size, valid_data_permutations],\n",
    "                                             output_types=((tf.float32, tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3), (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetTrainDataset = tf.data.Dataset.from_generator(dataset_permutations_generator,\n",
    "                                             args=[batch_size, train_data_permutations],\n",
    "                                             output_types=((tf.float32, tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3), (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1)\n",
    "                                                 #profile_batch = '1,3')\n",
    "except AlreadyExistsError:\n",
    "    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"/qarr/studia/magister/models/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del(modelUnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.fit(unetTrainDataset, \n",
    "              epochs=2, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=valid_cases//batch_size,\n",
    "              #callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) # batch_size unspecified since it's generated by generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.save('unet_test_model_small_02.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza wyników na przykładach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unetTrainDataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_result = modelUnet.predict(example)\n",
    "example = list(example.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = plt.subplots(1,4)\n",
    "subs = subs[0].axes\n",
    "subs[0].imshow(example[0][0][1][0])\n",
    "subs[1].imshow(example[0][0][0][0])\n",
    "subs[2].imshow(np.reshape(example[0][1][0], (256,256)), cmap='gist_gray')\n",
    "subs[3].imshow(np.reshape(example_result[0], (256,256)), cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytanie zapisanej sieci i kompilacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet = tf.keras.models.load_model(\"unet_test_model03.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") # weight decay 0.0005 by L2\n",
    "\n",
    "modelUnet.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\"\n",
    "),\n",
    "              #loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryCrossentropy(\n",
    "                        name=\"binary_crossentropy\")]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nowy callback do logowania gradientu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def _log_gradients(self, epoch):\n",
    "        writer = self._get_writer(self._train_run_name)\n",
    "\n",
    "        with writer.as_default(), tf.GradientTape() as g:\n",
    "            # here we use test data to calculate the gradients\n",
    "## Wrong approach dont do that, use internal variables of actual data not external lists\n",
    "            features, y_true = list(val_dataset.batch(100).take(1))[0]\n",
    "\n",
    "            y_pred = self.model(features)  # forward-propagation\n",
    "            loss = self.model.compiled_loss(y_true=y_true, y_pred=y_pred)  # calculate loss\n",
    "            gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\n",
    "\n",
    "            # In eager mode, grads does not have name, so we get names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # This function overwrites the on_epoch_end in tf.keras.callbacks.TensorBoard\n",
    "        # but we do need to run the original on_epoch_end, so here we use the super function.\n",
    "        super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "  def _log_gradients(self, epoch):\n",
    "    step = tf.cast(tf.math.floor((epoch+1)*num_instance/batch_size), dtype=tf.int64)\n",
    "    writer = self._get_writer(self._train_run_name)\n",
    "\n",
    "    with writer.as_default(), tf.GradientTape() as g:\n",
    "      # here we use test data to calculate the gradients\n",
    "      _x_batch = x_te[:100]\n",
    "      _y_batch = y_te[:100]\n",
    "\n",
    "      g.watch(_x_batch)\n",
    "      _y_pred = self.model(_x_batch)  # forward-propagation\n",
    "      loss = self.model.loss(y_true=_y_batch, y_pred=_y_pred)  # calculate loss\n",
    "      gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\n",
    "\n",
    "      # In eager mode, grads does not have name, so we get names from model.trainable_weights\n",
    "      for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "        tf.summary.histogram(\n",
    "            weights.name.replace(':', '_')+'_grads', data=grads, step=step)\n",
    "    \n",
    "    writer.flush()\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):  \n",
    "    # This function overwrites the on_epoch_end in tf.keras.callbacks.TensorBoard\n",
    "    # but we do need to run the original on_epoch_end, so here we use the super function. \n",
    "    super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "    if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "      self._log_gradients(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gradient Function\n",
    "epoch_gradient = []\n",
    "\n",
    "def get_gradient_func(model):\n",
    "    grads = tf.keras.backend.gradients(model.total_loss, model.trainable_weights)\n",
    "    # grads = K.gradients(model.loss, model.trainable_weights)\n",
    "    # inputs = model.model.inputs + model.targets + model.sample_weights\n",
    "    # use below line of code if above line doesn't work for you\n",
    "    # inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    func = tf.keras.backend.function(inputs, grads)\n",
    "    return func\n",
    "\n",
    "# Define the Required Callback Function\n",
    "class GradientCalcCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)\n",
    "        get_gradient = get_gradient_func(model)\n",
    "        grads = get_gradient([test_images, test_labels, np.ones(len(test_labels))])\n",
    "        \n",
    "        epoch_gradient.append(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gradient Function\n",
    "epoch_gradient = []\n",
    "\n",
    "def get_gradient_func(model):\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    # grads = K.gradients(model.loss, model.trainable_weights)\n",
    "    # inputs = model.model.inputs + model.targets + model.sample_weights\n",
    "    # use below line of code if above line doesn't work for you\n",
    "    # inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    func = K.function(inputs, grads)\n",
    "    return func\n",
    "\n",
    "# Define the Required Callback Function\n",
    "class GradientCalcCallback(tf.keras.callbacks.TensorBoard):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      get_gradient = get_gradient_func(model)\n",
    "      grads = get_gradient([test_images, test_labels, np.ones(len(test_labels))])\n",
    "      epoch_gradient.append(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(model, x_tensor):\n",
    "    #x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x_tensor)\n",
    "        loss = model(x_tensor)\n",
    "    return t.gradient(loss, x_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = modelUnet.evaluate(tdat)\n",
    "describe(tdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tf.Tensor(tdat[0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(modelUnet, unetDataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet(tdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
