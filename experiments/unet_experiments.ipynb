{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/qarr/studia/magister/models/logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytywanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MASKS_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/masks/'\n",
    "INPUT_PATH = '/qarr/studia/magister/datasets/FlickrLogos-v2/classes/jpg/'\n",
    "\n",
    "classes = [o for o in os.listdir(INPUT_PATH) if os.path.isdir(INPUT_PATH + '/' + o)]\n",
    "classes = [o for o in classes if o != 'no-logo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict()\n",
    "targets = dict()\n",
    "queries = dict()\n",
    "start_time = time.time()\n",
    "\n",
    "def rescale(nparray, scale=255.0):\n",
    "    return np.array(nparray, dtype=np.float32)/scale\n",
    "\n",
    "for c in classes:\n",
    "    root_input = INPUT_PATH + '/' + c \n",
    "    root_masks = MASKS_PATH + '/' + c\n",
    "    images[c] = list()\n",
    "    targets[c] = list()\n",
    "    queries[c] = list()\n",
    "    \n",
    "    for f in os.listdir(root_input):\n",
    "        img = cv2.imread(f'{root_input}/{f}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f'{root_masks}/{f}.mask.merged.png', cv2.IMREAD_GRAYSCALE)\n",
    "        bboxes = []\n",
    "        \n",
    "        with open(f'{root_masks}/{f}.bboxes.txt') as csvfile:\n",
    "            bboxread = csv.reader(csvfile, delimiter=' ')\n",
    "            next(bboxread)\n",
    "            for row in bboxread:\n",
    "                bboxes.append(row)\n",
    "                \n",
    "        for bbox in bboxes:\n",
    "            x,y,w,h = [int(i) for i in bbox]\n",
    "            imgslice = img[y:y+h, x:x+w]\n",
    "            imgslice = cv2.resize(imgslice, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "            queries[c].append(rescale(imgslice, 255.0))\n",
    "            # Biore tylko pierwszy z dostepnych bbox na obrazku\n",
    "            break \n",
    "            \n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        mask = cv2.resize(mask, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "        images[c].append(rescale(img, 255.0))\n",
    "        targets[c].append(rescale(mask, 255.0))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time-start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(13371337)\n",
    "\n",
    "nclasses = len(classes)\n",
    "nlogos = sum([len(images[c]) for c in classes])//nclasses\n",
    "all_cases = nclasses*nlogos*(nlogos-1)\n",
    "valid_cases = ((all_cases//batch_size)//10)*batch_size\n",
    "valid_unique_n = 2\n",
    "valid_unique_pairs = nclasses*2*np.sum(range(nlogos-1, nlogos-valid_unique_n-1, -1))\n",
    "\n",
    "train_data_permutations = np.zeros((all_cases-valid_cases, 3), dtype=np.int8)\n",
    "valid_data_permutations = np.zeros((valid_cases, 3), dtype=np.int8)\n",
    "\n",
    "skips = np.sort(rnd.choice(all_cases-valid_cases, size=valid_cases-valid_unique_pairs+1, replace=False))\n",
    "skips[-1] = all_cases\n",
    "\n",
    "trainIt = 0\n",
    "validIt = 0\n",
    "skipIt = 0\n",
    "for c_i in range(nclasses):\n",
    "    valid_unique = rnd.choice(nlogos, size=valid_unique_n, replace=False)\n",
    "    for n_i in range(nlogos):\n",
    "        for l_i in range(nlogos):\n",
    "            if n_i == l_i:\n",
    "                continue\n",
    "            if n_i in valid_unique or l_i in valid_unique:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "            elif skips[skipIt] == trainIt:\n",
    "                valid_data_permutations[validIt] = (c_i, n_i, l_i)\n",
    "                validIt += 1\n",
    "                skipIt += 1\n",
    "            else:\n",
    "                train_data_permutations[trainIt] = (c_i, n_i, l_i)                \n",
    "                trainIt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 200\n",
    "train_data_permutations = rnd.permutation(train_data_permutations)\n",
    "train_data_permutations = train_data_permutations[:(len(train_data_permutations)//factor//batch_size)*batch_size]\n",
    "valid_data_permutations = rnd.permutation(valid_data_permutations)\n",
    "valid_data_permutations = valid_data_permutations[:(len(valid_data_permutations)//factor//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3)),\n",
    "                       np.reshape(outquery, (batch_size, 64, 64, 3))\n",
    "                      ), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_permutations_generator_dummy(batch_size, data_permutations, repeat=True, shuffle=True):\n",
    "    s = 0\n",
    "    outimage = []\n",
    "    outquery = []\n",
    "    outtarget = []\n",
    "    loop = True\n",
    "    while loop:\n",
    "        if shuffle:\n",
    "            data_permutations = np.random.permutation(data_permutations)\n",
    "        for class_number, image_number, query_number in data_permutations:\n",
    "            c = classes[class_number]\n",
    "            outimage.append(images[c][image_number])\n",
    "            outquery.append(queries[c][query_number])\n",
    "            outtarget.append(targets[c][image_number])\n",
    "            s += 1\n",
    "            if s >= batch_size:\n",
    "                s = 0\n",
    "                yield (np.reshape(outimage, (batch_size, 256, 256, 3))), np.reshape(outtarget, (batch_size, 256, 256, 1))\n",
    "                outimage = []\n",
    "                outquery = []\n",
    "                outtarget = []\n",
    "    loop = repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetValidDataset = tf.data.Dataset.from_generator(dataset_permutations_generator_dummy,\n",
    "                                             args=[batch_size, valid_data_permutations],\n",
    "                                             output_types=((tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3)),# (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetTrainDataset = tf.data.Dataset.from_generator(dataset_permutations_generator_dummy,\n",
    "                                             args=[batch_size, train_data_permutations],\n",
    "                                             output_types=((tf.float32), tf.float32),\n",
    "                                             output_shapes=(((batch_size, 256,256,3)),# (batch_size, 64,64,3)),\n",
    "                                                          (batch_size, 256,256,1))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom gradients logging callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientsLoggerTBCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, gradient_reference, *args, **kwargs):\n",
    "        super(GradientsLoggerTBCallback, self).__init__(*args, **kwargs)\n",
    "        self._gradient_ref = gradient_reference\n",
    "        self.gradient_logs = []\n",
    "        self._epoch = 1\n",
    "        self.once = True\n",
    "        \n",
    "    def _get_gradient(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(self._gradient_ref[0], training=True)  # Forward pass\n",
    "            loss = self.model.compiled_loss(y_true=self._gradient_ref[1], y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        return gradients\n",
    "\n",
    "    def _log_gradients(self, epoch):\n",
    "        # changes in version \"2.2.0\"\n",
    "        if tf.version.VERSION.split('.')[1] <= '2':\n",
    "            writer = self._get_writer(self._train_run_name)\n",
    "        else:\n",
    "            writer = self._train_writer\n",
    "        gradients = self._get_gradient()\n",
    "        self.gradient_logs.append(gradients)\n",
    "        \n",
    "        with writer.as_default():\n",
    "            # Getting names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "            writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(GradientsLoggerTBCallback, self).on_epoch_end(epoch, logs=logs)\n",
    "        \n",
    "        self._epoch += 1\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)\n",
    "    \n",
    "    #def on_train_batch_end(self, batch, logs=None):\n",
    "        #if self.histogram_freq and self._epoch % self.histogram_freq == 0 and self.once:\n",
    "            #print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "            #print(self.__dir__())\n",
    "            #print(logs)\n",
    "            #self.once = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Stałe i parametry warstw splotowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightDecay = 0.0000000 # originally 5e-4\n",
    "momentum = 0.9\n",
    "learningRate = 0.004\n",
    "L2penalty = weightDecay/learningRate\n",
    "experimentName = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convOptions = {\n",
    "    \"strides\": 1,\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "\n",
    "convTransOptions = {\n",
    "    \"strides\": (2,2),\n",
    "    \"padding\": 'SAME', \n",
    "    \"activation\": tf.nn.relu,\n",
    "    #\"kernel_regularizer\": regularizers.l2(L2penalty),\n",
    "    #\"bias_regularizer\": regularizers.l2(L2penalty),\n",
    "    \"kernel_initializer\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "    #\"bias_initalizer\": tf.keras.initializers.Zeros()\n",
    "}\n",
    "\n",
    "maxPoolOptions = {\n",
    "    \"pool_size\": 2,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": 'SAME'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp1_wo_cond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część transenkodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None, 256, 256, None, 512, 512, None, 512, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część conditional branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputQuery  = tf.keras.Input(dtype = tf.float32, shape = [64, 64, 3], name = 'Query')\n",
    "layersConditionalEncoder = [LInputQuery]\n",
    "with tf.name_scope(\"Conditional\"):\n",
    "    filtersNumber=[32, 32, None, 64, 64, None, 128, None, 256, None, 512, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "        else:\n",
    "            layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    layersConditionalEncoder.append(\n",
    "                tf.keras.layers.Conv2D(filters = 512, \n",
    "                                       kernel_size=2, \n",
    "                                       strides=2, \n",
    "                                       **{k:v for k,v in convOptions.items() if k != 'strides'}\n",
    "                                      )(layersConditionalEncoder[-1])\n",
    "            )\n",
    "    conditionalEncoderOutput = layersConditionalEncoder[-1]\n",
    "\n",
    "    modelConditional = tf.keras.Model(\n",
    "        inputs=LInputQuery,\n",
    "        outputs=conditionalEncoderOutput, \n",
    "        name=\"Latent Representation Encoder\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Część transdekodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    tiles = [(8,8), (16,16), (32, 32), (64, 64), (128,128)]\n",
    "    filters = [(None, 512, 512, 512), \n",
    "               (512, 512, 512, 512), \n",
    "               (256, 256, 256, 256), \n",
    "               (128, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for tile, encodedInput, fs in zip(tiles, reversed(transcoderInputs), filters):\n",
    "        # Tiling output from conditional encoder\n",
    "        #layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=tile)(conditionalEncoderOutput))\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnet = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompilacja modelu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnet.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                          label_smoothing=0, \n",
    "                                                          reduction=\"auto\", \n",
    "                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy_metric\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "                                                                   #profile_batch = '1,3')\n",
    "except AlreadyExistsError:\n",
    "    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnet.fit(unetTrainDataset, \n",
    "              epochs=7, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=len(valid_data_permutations)//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp2_wo_cond_2layer\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None, 128, 128, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    filters = [(None, 128, 128, 128),\n",
    "               (64, 64, 64, 64)]\n",
    "    \n",
    "    for encodedInput, fs in zip(reversed(transcoderInputs), filters):\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnetEx2 = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnetEx2.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                          label_smoothing=0, \n",
    "                                                          reduction=\"auto\", \n",
    "                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy_metric\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logdir = \"/qarr/studia/magister/models/logs/\"\n",
    "logs = logdir + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "except AlreadyExistsError:\n",
    "    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUnetEx2.fit(unetTrainDataset, \n",
    "              epochs=3, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=len(valid_data_permutations)//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp3_wo_cond_1layer\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            #layersEncoder.append(tf.keras.layers.BatchNormalization()(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    filters = [(None, 64, 64, 64)]\n",
    "    \n",
    "    for encodedInput, fs in zip(reversed(transcoderInputs), filters):\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        #layersTransDecoder.append(tf.keras.layers.BatchNormalization()(layersTransDecoder[-1]))\n",
    "\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        #layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=(2,2))(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "\n",
    "    modelUnetEx3 = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnetEx3.compile(optimizer=optimizer,\n",
    "                  loss=custom_bce_loss,#tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=\"sum\"),#\n",
    "#                                                          label_smoothing=0, \n",
    "#                                                          reduction=\"sum_over_batch_size\", \n",
    "#                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[#tf.keras.metrics.BinaryCrossentropy(name=\"bin_c\", from_logits=False),\n",
    "                           #tf.keras.metrics.MeanIoU(2, name=None, dtype=None),\n",
    "                           tf.keras.metrics.TruePositives(name=\"TP\"),\n",
    "                           tf.keras.metrics.TrueNegatives(name=\"TN\"),\n",
    "                           tf.keras.metrics.FalsePositives(name=\"FP\"),\n",
    "                           tf.keras.metrics.FalseNegatives(name=\"FN\"),\n",
    "                           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(modelUnetEx3, \"obrazek.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "finally:\n",
    "    pass\n",
    "#except AlreadyExistsError:\n",
    "#    print(\"Already exists, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "#sess = tf.compat.v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    modelUnetEx3.fit(unetTrainDataset, \n",
    "              epochs=5, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=len(valid_data_permutations)//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) \n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    try:\n",
    "        return f'{x.shape}'\n",
    "    except AttributeError:\n",
    "        return '[' + ', \\n'.join([describe(q) for q in x]) + ']'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe(tboard_callback.gradient_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.name for x in modelUnetEx3.trainable_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.array(tboard_callback.gradient_logs[0][0].as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1 = tboard_callback.gradient_logs[0][-2].numpy()\n",
    "grad_1 = np.ravel(grad_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(grad_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = modelUnetEx3.trainable_weights[0].numpy()\n",
    "print(weights_1.shape)\n",
    "weights_1 = np.ravel(weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weights_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = modelUnetEx3.predict(unetTrainDataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.ravel(ev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "losses = [\n",
    "    tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.NONE),\n",
    "    tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrange = np.linspace(0,1,1000)\n",
    "xx, yy = np.meshgrid(testrange, testrange*4-2)\n",
    "xx3 = np.reshape(xx, (1000, 1000, 1))\n",
    "yy3 = np.reshape(yy, (1000, 1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_bce_loss(y_true, y_pred, ratio = 0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    dtype = y_pred.dtype.base_dtype\n",
    "    epsilon = tf.keras.backend.epsilon\n",
    "    shape = y_true.shape\n",
    "    pixels = np.prod(shape)\n",
    "    \n",
    "    mask = tf.greater_equal(y_true, 0.5)\n",
    "    mask = tf.cast(mask, dtype)\n",
    "    whites = tf.math.count_nonzero(mask, dtype=dtype) # sum?\n",
    "    whites_weight = ratio * pixels / (whites + epsilon())\n",
    "    blacks_weight = (1 - ratio) * pixels / (pixels - whites + epsilon())\n",
    "    \n",
    "    mask = tf.multiply(mask, whites_weight - blacks_weight)\n",
    "    mask = tf.add(mask, blacks_weight)\n",
    "    \n",
    "    # mean over whole batch\n",
    "    \n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    \n",
    "    #epsilon_ = _constant_to_tensor(epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon(), 1. - epsilon())\n",
    "    \n",
    "    # With logits\n",
    "    bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    \n",
    "    # Without logits\n",
    "    # Compute cross entropy from probabilities.\n",
    "    #bce = y_true * tf.math.log(y_pred + epsilon())\n",
    "    #bce += (1 - y_true) * tf.math.log(1 - y_pred + epsilon())\n",
    "    bce = tf.multiply(bce, mask)\n",
    "    #bce = tf.reduce_sum(bce, axis=range(0,len(shape)))\n",
    "    return bce #-bce # without logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBCELoss(tf.python.keras.losses.LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "                reduction=tf.keras.losses.Reduction.AUTO,\n",
    "                name='custom_bce_loss', ratio=0.5):\n",
    "\n",
    "        super(CustomBCELoss, self).__init__(custom_bce_loss, name=name, reduction=reduction, ratio=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unetTrainDataset.take(1)\n",
    "example = next(example.as_numpy_iterator())\n",
    "out = modelUnetEx3.predict(example[0])\n",
    "\n",
    "# Calculating gradient using custom bce loss function\n",
    "with tf.GradientTape() as tape:\n",
    "    out = modelUnetEx3(example[0], training=True)\n",
    "    loss = custom_bce_loss(out, example[1])\n",
    "gradients1 = tape.gradient(loss, modelUnetEx3.trainable_weights)\n",
    "\n",
    "# Calculating gradient using standard bce loss function\n",
    "with tf.GradientTape() as tape:\n",
    "    out = modelUnetEx3(example[0])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)(out, example[1])\n",
    "gradients2 = tape.gradient(loss, modelUnetEx3.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The histograms from both functions should differ\n",
    "plt.hist([np.ravel(gradients1[0]), np.ravel(gradients2[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3\n",
    "subs = plt.subplots(1,3)\n",
    "subs = subs[0].axes\n",
    "subs[0].imshow(example[0][num])\n",
    "subs[1].imshow(np.reshape(example[1][num], (256,256)), cmap='gist_gray')\n",
    "subs[2].imshow(np.reshape(out[num], (256,256)), cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = custom_bce_loss(xx3, yy3, 0.5)\n",
    "zz = np.reshape(zz, (1000,1000))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax.plot_surface(xx, yy, zz, cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pairs = np.reshape(np.array([(i, j) for i in testrange for j in testrange]), (1000,1000,-1))\n",
    "gt = np.repeat(np.repeat([[[0,1]]], (1000), axis=1), 1000, axis=0)\n",
    "gt.shape\n",
    "zz = losses[2](gt, pairs)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax.plot_surface(xx, np.transpose(xx), zz, cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "    zz = loss(xx3, yy3)\n",
    "    nonNanMask = np.logical_not(np.isnan(zz.numpy()).any(axis=1))\n",
    "    zz = zz[nonNanMask]\n",
    "    ax = fig.add_subplot(3,3,i+1, projection='3d')\n",
    "    ax.set_title(type(losses[i]).__name__)\n",
    "    ax.plot_surface(xx[nonNanMask], yy[nonNanMask], zz, cmap=\"jet\", rcount=30, ccount=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "def init():\n",
    "    return fig,\n",
    "\n",
    "def update(frame):\n",
    "    for ax in fig.axes:\n",
    "        ax.view_init(elev=5., azim=frame)\n",
    "    #ln.set_data(xdata, ydata)\n",
    "    return fig,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 360, 60),\n",
    "                    init_func=init)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, scale, bias=0.0):\n",
    "        super(SigmoidLayer, self).__init__(trainable=False)\n",
    "        #self.scale = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "        self.scale = scale\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return tf.math.sigmoid(tf.math.add(tf.math.multiply(inputs, self.scale), self.bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SigmoidLayer(2*1.3862943611198906, -1.3862943611198906)([-1.0, 0.0, 1.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = \"exp5_simple\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LInputTarget = tf.keras.Input(dtype = tf.float32, shape = [256, 256, 3], name = 'Target')\n",
    "layersEncoder = [LInputTarget]\n",
    "transcoderInputs = []\n",
    "with tf.name_scope(\"Encoder\"):\n",
    "    filtersNumber=[64, 64, None]\n",
    "    for fn in filtersNumber:\n",
    "        if fn is None:\n",
    "            layersEncoder.append(tf.keras.layers.MaxPool2D(**maxPoolOptions)(layersEncoder[-1]))\n",
    "            #layersEncoder.append(tf.keras.layers.BatchNormalization()(layersEncoder[-1]))\n",
    "            transcoderInputs.append(layersEncoder[-1])\n",
    "        else:\n",
    "            layersEncoder.append(tf.keras.layers.Conv2D(filters = fn, kernel_size=3, **convOptions)(layersEncoder[-1]))\n",
    "    encoderOutput = layersEncoder[-1]\n",
    "    \n",
    "    modelEncoder = tf.keras.Model(\n",
    "        inputs=LInputTarget, \n",
    "        outputs=encoderOutput,\n",
    "        name=\"Encoder model\"\n",
    "    )\n",
    "    \n",
    "with tf.name_scope(\"Transcoder\"):\n",
    "    layersTransDecoder = []\n",
    "    upsampledLayers = []\n",
    "    filters = [(None, 64, 64, 64)]\n",
    "    \n",
    "    for encodedInput, fs in zip(reversed(transcoderInputs), filters):\n",
    "        # Concatenating tiled output with reverse order of encoder MaxPool layers\n",
    "        layersTransDecoder.append(encodedInput)\n",
    "        # Flattening the concatenation with 1x1 conv if needed and joining with last cycle's result\n",
    "        if fs[0] is not None:\n",
    "            layersTransDecoder.append(tf.keras.layers.Conv2D(fs[0], kernel_size=1, **convOptions)(layersTransDecoder[-1]))\n",
    "            layersTransDecoder.append(tf.keras.layers.Concatenate()([layersTransDecoder[-1], upsampledLayers[-1]]))\n",
    "        # Transdecoding encoded values with Conv2D layers\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[1], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2D(fs[2], kernel_size=3, **convOptions)(layersTransDecoder[-1]))\n",
    "        #layersTransDecoder.append(tf.keras.layers.BatchNormalization()(layersTransDecoder[-1]))\n",
    "\n",
    "        # Upsampling with transposed convolution filters, saving the layer for next cycle merging\n",
    "        layersTransDecoder.append(tf.keras.layers.Conv2DTranspose(fs[3], kernel_size=3, **convTransOptions)(layersTransDecoder[-1]))\n",
    "        #layersTransDecoder.append(tf.keras.layers.UpSampling2D(size=(2,2))(layersTransDecoder[-1]))\n",
    "        upsampledLayers.append(layersTransDecoder[-1])\n",
    "        \n",
    "    unetOutput = tf.keras.layers.Conv2D(filters = 1, kernel_size=3, **convOptions, name=\"Output\")(layersTransDecoder[-1])\n",
    "    layersTransDecoder.append(unetOutput)\n",
    "    unetOutput = SigmoidLayer(2*1.3862943611198906, -1.3862943611198906)(unetOutput)\n",
    "\n",
    "    modelUnetEx5 = tf.keras.Model(inputs=[LInputTarget], outputs=[unetOutput], name=\"Unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=momentum, nesterov=False, name=\"SGD\") \n",
    "# weight decay 0.0005 by L2\n",
    "\n",
    "modelUnetEx5.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=\"sum\"),#custom_bce_loss,#\n",
    "#                                                          label_smoothing=0, \n",
    "#                                                          reduction=\"sum_over_batch_size\", \n",
    "#                                                          name=\"binary_crossentropy\"),\n",
    "                  metrics=[#tf.keras.metrics.BinaryCrossentropy(name=\"bin_c\", from_logits=False),\n",
    "                           #tf.keras.metrics.MeanIoU(2, name=None, dtype=None),\n",
    "                           tf.keras.metrics.TruePositives(name=\"TP\"),\n",
    "                           tf.keras.metrics.TrueNegatives(name=\"TN\"),\n",
    "                           tf.keras.metrics.FalsePositives(name=\"FP\"),\n",
    "                           tf.keras.metrics.FalseNegatives(name=\"FN\"),\n",
    "                           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "logs = \"/qarr/studia/magister/models/logs/\" + experimentName + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "try:\n",
    "    tboard_callback = GradientsLoggerTBCallback(list(unetValidDataset.take(1))[0],\n",
    "                                                                   log_dir = logs,\n",
    "                                                                   histogram_freq = 1)\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    modelUnetEx5.fit(unetTrainDataset, \n",
    "              epochs=5, \n",
    "              steps_per_epoch=len(train_data_permutations)//batch_size, \n",
    "              validation_data=unetValidDataset,\n",
    "              validation_steps=len(valid_data_permutations)//batch_size,\n",
    "              callbacks=[tboard_callback]#, callbackCheckpoint]\n",
    "             ) \n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
