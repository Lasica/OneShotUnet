{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to unzip zipped file, failed\n",
    "#import gzip\n",
    "#with gzip.open('fashion_mnist/train-images-idx3-ubyte.gz') as f:\n",
    "\n",
    "learning_set = np.fromfile('fashion_mnist/train-images-idx3-ubyte', dtype='uint8')\n",
    "learning_lab = np.fromfile('fashion_mnist/train-labels-idx1-ubyte', dtype='uint8')\n",
    "test_set = np.fromfile('fashion_mnist/t10k-images-idx3-ubyte', dtype='uint8')\n",
    "test_lab = np.fromfile('fashion_mnist/t10k-labels-idx1-ubyte', dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']\n",
    "print(learning_set[4:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing MNIST data into separate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PIL for image manipulation\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])  \n",
    "img = np.reshape(learning_set[range(16,28*28+16)], (28,28))\n",
    "plt.imshow(img, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(learning_set[16:], (60000, 28, 28, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data straight from folders, will work in 2.4.0\n",
    "\n",
    "# fashion_mnist_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     train_path,\n",
    "#     labels=\"inferred\",import tensorflow as tf\n",
    "\n",
    "#     label_mode=\"int\",\n",
    "#     class_names=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot'],\n",
    "#     color_mode='grayscale',\n",
    "#     batch_size=32,\n",
    "#     image_size=(28, 28),\n",
    "#     shuffle=True,\n",
    "#     seed=None,\n",
    "#     validation_split=None,\n",
    "#     subset=None,\n",
    "#     interpolation=\"bilinear\",\n",
    "#     follow_links=False,\n",
    "# )\n",
    "\n",
    "# fashion_mnist_test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     test_path,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"int\",\n",
    "#     class_names=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot'],\n",
    "#     color_mode='grayscale',\n",
    "#     batch_size=32,\n",
    "#     image_size=(28, 28),\n",
    "#     shuffle=True,\n",
    "#     seed=None,\n",
    "#     validation_split=None,\n",
    "#     subset=None,\n",
    "#     interpolation=\"bilinear\",\n",
    "#     follow_links=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "train_examples = 60000\n",
    "test_examples = 10000\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.reshape(learning_set[16:], (60000, 28, 28, 1)), # Input\n",
    "     learning_lab[8:]) # Output\n",
    ")\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.reshape(test_set[16:], (10000, 28, 28, 1)), # Input\n",
    "     test_lab[8:]) # Output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether dataset was loaded properly, should show a boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])  \n",
    "img = np.reshape(train_dataset.as_numpy_iterator().next()[0], (28,28))\n",
    "plt.imshow(img, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loaded, learning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #layerInput = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3,3), activation=tf.nn.relu,\n",
    "#                            input_shape=(28, 28, 1)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "#     tf.keras.layers.Dropout(.1, input_shape=(28,28,32*64)),\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "#     tf.keras.layers.Dropout(.1, input_shape=(28,28,32*64)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "    \n",
    "#layerInput = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "Linput = tf.keras.Input(dtype = tf.float32, shape = (28, 28, 1), name = 'Target')\n",
    "prevLayer = tf.keras.layers.Conv2D(32, (3,3), activation=tf.nn.relu, input_shape=(28, 28, 1))(Linput)\n",
    "prevLayer = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(prevLayer)\n",
    "prevLayer = tf.keras.layers.Dropout(.1, input_shape=(28,28,32*64))(prevLayer)\n",
    "prevLayer = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu)(prevLayer)\n",
    "prevLayer = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(prevLayer)\n",
    "prevLayer = tf.keras.layers.Dropout(.1, input_shape=(28,28,32*64))(prevLayer)\n",
    "prevLayer = tf.keras.layers.Flatten()(prevLayer)\n",
    "prevLayer = tf.keras.layers.Dense(128, activation=tf.nn.relu)(prevLayer)\n",
    "prevLayer = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(prevLayer)\n",
    "\n",
    "model = tf.keras.Model(inputs = Linput, outputs = prevLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.cache().repeat().shuffle(train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=3, steps_per_epoch=math.ceil(train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_labels, images):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  \n",
    "  plt.imshow(img[...,0], cmap=plt.cm.binary)\n",
    "    \n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "  \n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_images, test_labels in test_dataset.take(1):\n",
    "  test_images = test_images.numpy()\n",
    "  test_labels = test_labels.numpy()\n",
    "  predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted label, and the true label\n",
    "# Color correct predictions in blue, incorrect predictions in red\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions, test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model saving/loading and progress tracking/profiling capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model): #if you're using a model as a layer\n",
    "            reset_weights(layer) #apply function recursively\n",
    "            continue\n",
    "\n",
    "        #where are the initializers?\n",
    "        if hasattr(layer, 'cell'):\n",
    "            init_container = layer.cell\n",
    "        else:\n",
    "            init_container = layer\n",
    "\n",
    "        for key, initializer in init_container.__dict__.items():\n",
    "            if \"initializer\" not in key: #is this item an initializer?\n",
    "                  continue #if no, skip it\n",
    "\n",
    "            # find the corresponding variable, like the kernel or the bias\n",
    "            if key == 'recurrent_initializer': #special case check\n",
    "                var = getattr(init_container, 'recurrent_kernel')\n",
    "            else:\n",
    "                var = getattr(init_container, key.replace(\"_initializer\", \"\"))\n",
    "\n",
    "            var.assign(initializer(var.shape, var.dtype))\n",
    "            #use the initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard callback for saving progress of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackTB = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=3,\n",
    "    embeddings_metadata=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback for saving values in csv format with CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackCSV = tf.keras.callbacks.CSVLogger(\"fashion-csvlog.csv\", separator=\",\", append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback for saving model when given conditions have been met. In this example when loss value reaches minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"fashion-model\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset, \n",
    "#           epochs=1, \n",
    "#           steps_per_epoch=math.ceil(train_examples/BATCH_SIZE), \n",
    "#           validation_data=test_dataset,\n",
    "#           callbacks=[callbackCheckpoint,\n",
    "#           callbackCSV,\n",
    "#           callbackTB]\n",
    "#          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard logging gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def _log_gradients(self, epoch):\n",
    "        writer = self._get_writer(self._train_run_name)\n",
    "\n",
    "        with writer.as_default(), tf.GradientTape() as g:\n",
    "            # here we use test data to calculate the gradients\n",
    "## Wrong approach dont do that, use internal variables of actual data not external lists\n",
    "            features, y_true = list(val_dataset.batch(100).take(1))[0]\n",
    "\n",
    "            y_pred = self.model(features)  # forward-propagation\n",
    "            loss = self.model.compiled_loss(y_true=y_true, y_pred=y_pred)  # calculate loss\n",
    "            gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\n",
    "\n",
    "            # In eager mode, grads does not have name, so we get names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # This function overwrites the on_epoch_end in tf.keras.callbacks.TensorBoard\n",
    "        # but we do need to run the original on_epoch_end, so here we use the super function.\n",
    "        super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_log = []\n",
    "\n",
    "class GradientsLoggerTBCallback(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, gradient_reference, *args, **kwargs):\n",
    "        super(GradientsLoggerTBCallback, self).__init__(*args, **kwargs)\n",
    "        self._gradient_ref = gradient_reference\n",
    "        self._epoch = 1\n",
    "        self.once = True\n",
    "        \n",
    "    def _get_gradient(self, model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(self._gradient_ref[0], training=True)  # Forward pass\n",
    "            loss = self.model.compiled_loss(y_true=self._gradient_ref[1], y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        return gradients\n",
    "\n",
    "    def _log_gradients(self, epoch):\n",
    "        writer = self._get_writer(self._train_run_name)\n",
    "        gradients = self._get_gradient(model)\n",
    "        \n",
    "        gradient_log.append(gradients)\n",
    "        with writer.as_default():\n",
    "            # Getting names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
    "        writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(GradientsLoggerTBCallback, self).on_epoch_end(epoch, logs=logs)\n",
    "        \n",
    "        self._epoch += 1\n",
    "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "            self._log_gradients(epoch)\n",
    "    \n",
    "    #def on_train_batch_end(self, batch, logs=None):\n",
    "        #if self.histogram_freq and self._epoch % self.histogram_freq == 0 and self.once:\n",
    "            #print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "            #print(self.__dir__())\n",
    "            #print(logs)\n",
    "            #self.once = False\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbackGradientsLogger = GradientsLoggerTBCallback(\n",
    "    list(test_dataset.take(1))[0],\n",
    "    log_dir=\"/qarr/studia/magister/models/logs\",\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=2, \n",
    "          steps_per_epoch=math.ceil(train_examples/BATCH_SIZE), \n",
    "          validation_data=test_dataset,\n",
    "          callbacks=[callbackGradientsLogger]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    try:\n",
    "        return f'{x.shape}'\n",
    "    except AttributeError:\n",
    "        return '[' + ', \\n'.join([describe(q) for q in x]) + ']'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe(gradient_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w.name for w in model.trainable_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
