{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import scipy.interpolate\n",
    "import re\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No compatible GPUs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stałe i ustawienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"siamese_logs/\" \n",
    "SAVE_PATH = \"/qarr/studia/magister/tekst/graphs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STEP=90\n",
    "winterCmap = matplotlib.cm.get_cmap(\"cool\")\n",
    "wistiaCmap = matplotlib.cm.get_cmap(\"autumn\")\n",
    "matplotlib.rcParams['axes.xmargin'] = 0\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['legend.fontsize'] = 'medium'\n",
    "matplotlib.rcParams['font.size'] = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def merge_tbevents(dirpath, tag):\n",
    "    eventfiles = [dirpath+\"/\"+x for x in os.listdir(dirpath)]\n",
    "    # Assumption - maximum of data entry per file\n",
    "    xx = []\n",
    "    yy = []\n",
    "    for n, evfile in enumerate(eventfiles):\n",
    "        itr = tf.compat.v1.train.summary_iterator(evfile)\n",
    "        for i in itr:\n",
    "            step = i.step\n",
    "            if i.summary and i.summary.value:\n",
    "                for v in i.summary.value:\n",
    "                    if v.tag == tag:\n",
    "                        f = struct.unpack('f', v.tensor.tensor_content)\n",
    "                        xx.append(step)\n",
    "                        yy.append(f[0])\n",
    "    xx = np.array(xx)\n",
    "    yy = np.array(yy)\n",
    "    order = np.argsort(xx)\n",
    "    xx = xx[order]\n",
    "    yy = yy[order]\n",
    "    return xx, yy\n",
    "\n",
    "def discover_tags(dirpath):\n",
    "    eventfiles = [dirpath+\"/\"+x for x in os.listdir(dirpath)]\n",
    "    tags = set()\n",
    "    for n, evfile in enumerate(eventfiles):\n",
    "        itr = tf.compat.v1.train.summary_iterator(evfile)\n",
    "        for i in itr:\n",
    "            step = i.step\n",
    "            if i.summary and i.summary.value:\n",
    "                for v in i.summary.value:\n",
    "                    tags.add(v.tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def smooth(x, series, weight, points=0):\n",
    "    smoothed = np.zeros(len(series))\n",
    "    smoothed[0] = series[0]\n",
    "    for i in range(1, len(series)):\n",
    "        smoothed[i] = series[i]*(1-weight) + weight*smoothed[i-1]\n",
    "    if points:\n",
    "        spline = cubic_interploation_model=scipy.interpolate.interp1d(x,smoothed,kind=\"cubic\")\n",
    "        xrange = np.linspace(x.min(), x.max(), num=points, endpoint=True, retstep=False, dtype=None, axis=0)\n",
    "        yrange = spline(xrange)\n",
    "        return (xrange, yrange)\n",
    "    else:\n",
    "        return (x, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     19,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "    \"\"\"Exponentially weighted moving average; taken from \n",
    "    https://stackoverflow.com/questions/42869495/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm\n",
    "    Has troubles with large datasets due to high power exponents\"\"\"\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "def window_size(alpha, sum_proportion):\n",
    "    # Increases with increased sum_proportion and decreased alpha\n",
    "    # solve (1-alpha)**window_size = (1-sum_proportion) for window_size        \n",
    "    return int(np.log(1-sum_proportion) / np.log(1-alpha))\n",
    "\n",
    "def smooth_ewma(x, series, w):\n",
    "    n = len(series)\n",
    "    # w*1= n/2\n",
    "    # w*0= 1\n",
    "    window = int((n/2-1)*w) + 1\n",
    "    smoothed = numpy_ewma_vectorized_v2(series, window)\n",
    "    return (x, smoothed)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss_graph(experiments, figure=None, title=\"\", alpha=1.0, legend=None, drawRaw=True, saveName=None, smoothFn=smooth, smoothParams=None):\n",
    "    exps = []\n",
    "    validationCount = 0\n",
    "    trainCount = 0\n",
    "    \n",
    "    if smoothParams == None:\n",
    "        smoothParams = [0.8]\n",
    "    \n",
    "    for ex in experiments:\n",
    "        s = re.sub(r\".*//\", \"\" , ex)\n",
    "        nameString, dsType = s.split(\"/\")\n",
    "        expName = re.sub(r\"_.*$\", \"\", nameString)\n",
    "        exps.append((dsType, expName, ex))\n",
    "        \n",
    "    colorStep = min(256/len(experiments)*2, COLOR_STEP)\n",
    "    \n",
    "    if figure is None:\n",
    "        fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "    else:\n",
    "        fig, axs = figure\n",
    "    #fig.tight_layout()\n",
    "    axs.set_title(title)\n",
    "    axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "    defaultLegend = []\n",
    "    for dsType, expName, expPath in exps:\n",
    "        print(dsType, expName, expPath)\n",
    "        if dsType == \"validation\":\n",
    "            color = wistiaCmap(int(colorStep*validationCount))\n",
    "            defaultLegend.append(expName + \" - valid\")\n",
    "            validationCount += 1\n",
    "        else:\n",
    "            color = winterCmap(256-int(colorStep*trainCount))\n",
    "            defaultLegend.append(expName + \" - train\")\n",
    "            trainCount += 1\n",
    "        \n",
    "        xx, yy = merge_tbevents(expPath, \"mean_loss\")\n",
    "        if drawRaw:\n",
    "            axs.plot(xx,yy, alpha=0.3*alpha, color=color)\n",
    "        axs.plot(*smoothFn(xx, yy, *smoothParams), alpha=alpha, color=color)\n",
    "\n",
    "    axs.set_xlabel(\"numer epoki\")\n",
    "    axs.set_ylabel(\"uśredniona funkcja straty z epoki\")\n",
    "    if legend is not None:\n",
    "        axs.legend(legend)\n",
    "    else:\n",
    "        axs.legend(defaultLegend)\n",
    "    if saveName:\n",
    "        print(f\"Saved {SAVE_PATH + saveName}\")\n",
    "        fig.savefig(SAVE_PATH + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_ranking_graph(experiments, title=\"\", legend=None, alpha=1.0, saveName=None, figure=None):\n",
    "    exps = []\n",
    "    validationCount = 0\n",
    "    trainCount = 0\n",
    "        \n",
    "    for ex in experiments:\n",
    "        s = re.sub(r\".*//\", \"\" , ex)\n",
    "        nameString, dsType = s.split(\"/\")\n",
    "        expName = re.sub(r\"_.*$\", \"\", nameString)\n",
    "        if dsType == \"validation\":\n",
    "            print(f\"Skipping {ex}, no data\")\n",
    "            continue\n",
    "        trainCount += 1\n",
    "        exps.append((dsType, expName, ex))\n",
    "        \n",
    "    colorStep = min(256/trainCount, COLOR_STEP)\n",
    "    defaultLegend = []\n",
    "    \n",
    "    if figure is None:\n",
    "        fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "    else:\n",
    "        fig, axs = figure\n",
    "    #fig.tight_layout()\n",
    "    axs.set_title(title)\n",
    "    axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "    \n",
    "    count = 0\n",
    "    for dsType, expName, expPath in exps:\n",
    "        print(dsType, expName, expPath)\n",
    "        color = winterCmap(256-int(colorStep*count))\n",
    "        defaultLegend.append(expName)\n",
    "        count += 1\n",
    "        \n",
    "        xx, yy = merge_tbevents(expPath, \"rank_normalised\")\n",
    "        axs.plot(xx,yy, alpha=alpha, color=color)\n",
    "\n",
    "        \n",
    "    axs.set_xlabel(\"numer epoki\")\n",
    "    axs.set_ylabel(\"znormalizowany ranking\")\n",
    "    if legend is None:\n",
    "        axs.legend(defaultLegend)\n",
    "    else:\n",
    "        axs.legend(legend)\n",
    "    if saveName:\n",
    "        print(f\"Saved {SAVE_PATH + saveName}\")\n",
    "        fig.savefig(SAVE_PATH + saveName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def filter_experiments(patterns, experimentsList):\n",
    "    return [e for f in patterns for e in experimentsList if re.search(f, e) is not None]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie sciezek eksperymentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(\"/home/zenfur/magister/jupyter/siamese_logs/\")\n",
    "root, dirs, _ = next(os.walk(LOG_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastExperiments = list()\n",
    "for d in dirs:\n",
    "    subdirs = os.listdir(root+d)\n",
    "    if \"validation\" in subdirs:\n",
    "        for sd in subdirs:\n",
    "            pastExperiments.append(f\"{root}/{d}/{sd}\")\n",
    "    elif \"train\" not in subdirs:\n",
    "        for dd in subdirs:\n",
    "            if os.path.isdir(f\"{root}/{d}/{dd}\"):\n",
    "                subsubdirs = os.listdir(root +  d + \"/\" + dd)\n",
    "                if \"validation\" in subsubdirs:\n",
    "                    for sd in subsubdirs:\n",
    "                        pastExperiments.append(root + d + \"//\" + dd + \"/\" + sd)\n",
    "    else:\n",
    "        print(f\"Omitting {d} experiment - no validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chronoOrder = np.argsort([e.split('/')[-2].split('_')[1] for e in pastExperiments])\n",
    "\n",
    "# with open(\"experiments.list\", \"w\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for chord in chronoOrder:\n",
    "#         splits = pastExperiments[chord].split('/')\n",
    "#         name, date = splits[-2].split('_')\n",
    "#         comment = \"\"\n",
    "#         writer.writerow((date, name, pastExperiments[chord], comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pastExperiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-044333\"], pastExperiments), \n",
    "                #title=\"Uśredniona funkcja straty eksperymentu 'baseline'\",\n",
    "               legend=[\"błąd zbioru trenującego\", \"wygładzony błąd zb. trenującego\", \"błąd zbioru walidacyjnego\", \"wygładzony błąd zb. walidacyjnego\"],\n",
    "               saveName=\"baseline_meanError_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-044333\"], pastExperiments),\n",
    "                         legend=[\"ranking baseline\"],\n",
    "                         alpha=0.3,\n",
    "                         saveName=\"baseline_normRanking_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"baselineLong\"], pastExperiments),\n",
    "                         legend=[\"ranking baseline\"],\n",
    "                         alpha=0.3,\n",
    "                         saveName=\"baseline_normRanking_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"baselineLong\"], pastExperiments), \n",
    "                #title=\"Uśredniona funkcja straty eksperymentu 'baseline'\",\n",
    "               legend=[\"błąd zbioru trenującego\", \"wygładzony błąd zb. trenującego\", \"błąd zbioru walidacyjnego\", \"wygładzony błąd zb. walidacyjnego\"],\n",
    "               saveName=\"baseline_meanError_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Eksperyment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-044333\", \"20210423-232523\", \"20210424-005223\"], pastExperiments), \n",
    "                #[pastExperiments[1], pastExperiments[0], pastExperiments[-1], pastExperiments[-2],  pastExperiments[7], pastExperiments[6]], \n",
    "                \"\",\n",
    "               None,#['baseline - training', 'baseline - valid.', 'output norm 3 - train', 'output norm 3 - valid.', 'output norm 2 - train', 'output norm 2 - valid.'],\n",
    "               drawRaw=False,\n",
    "               saveName=\"output-normalisations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-044333\", \"20210423-232523\", \"20210424-005223\"], pastExperiments),\n",
    "                \"\",\n",
    "               None,#['baseline', 'output norm 3', 'output norm 2'],\n",
    "               alpha=1,\n",
    "               saveName=\"output-normalisations-ranks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Eksperyment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-044333\", \"20210424-023006\", \"20210424-031711\", \"20210424-001214\"], pastExperiments),\n",
    "                \"\",\n",
    "               None,\n",
    "               drawRaw=False,\n",
    "               saveName=\"different-sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-044333\", \"20210424-023006\", \"20210424-031711\", \"20210424-001214\"], pastExperiments),\n",
    "                                            alpha=1, saveName=\"different-sizes-ranks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Eksperyment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-001214\", \"20210520-194658\", \"20210520-190857\", \"20210520-183105\"], pastExperiments),\n",
    "                drawRaw=False,\n",
    "                saveName=\"different-sizes-v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-001214\", \"20210520-194658\", \"20210520-190857\", \"20210520-183105\"], pastExperiments),\n",
    "                        alpha=1.0,\n",
    "                        saveName=\"different-sizes-v2-ranks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Eksperyment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selection = [e for e in pastExperiments if ('batch_03' in e or 'batch_04' in e or 'batch_05' in e) and \"baseline_\" in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "errors = [merge_tbevents(s, \"mean_loss\")[-1][-1] for s in selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selectionV = selection[::2]\n",
    "selectionT = selection[1::2]\n",
    "errors = errors[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "order = np.argsort(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selection = [selectionV[x] for x in order] + [selectionT[x] for x in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figpair = plt.subplots(1,1, figsize=(8,6))\n",
    "mean_loss_graph(selection, figure = figpair, legend=[], drawRaw=False)\n",
    "baseline = filter_experiments([\"20210424-044333/validation\"], pastExperiments)\n",
    "baseline = merge_tbevents(baseline[0], \"mean_loss\")\n",
    "line = figpair[1].plot(*smooth(baseline[0], baseline[1], 0.8), color=[0,0,0], linestyle=\"dotted\")\n",
    "figpair[1].legend(line, [\"baseline - referencyjny\"])\n",
    "figpair[0].savefig(SAVE_PATH +\"baselines-spread.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figpair = plt.subplots(1,1, figsize=(8,6))\n",
    "baseline = filter_experiments([\"20210424-044333/train\"], pastExperiments)\n",
    "baseline = merge_tbevents(baseline[0], \"rank_normalised\")\n",
    "normalised_ranking_graph(selection, figure=figpair, legend=[], alpha=1)\n",
    "line = figpair[1].plot(*baseline, color=[0,0,0], linestyle=\"dotted\")\n",
    "figpair[1].legend(line, [\"baseline - referencyjny\"])\n",
    "figpair[0].savefig(SAVE_PATH +\"baselines-spread-ranks.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = merge_tbevents(selectionT[0], \"rank_normalised\")[0]\n",
    "rankings = [merge_tbevents(s, \"rank_normalised\")[1] for s in selectionT]\n",
    "rankings = np.array(rankings)\n",
    "mean_ranking = np.mean(rankings, axis=0)\n",
    "confidence95 = 1.96*np.sqrt(np.var(rankings, axis=0))\n",
    "\n",
    "figpair = plt.subplots(1,1, figsize=(8,6))\n",
    "l1 = figpair[1].plot(x, mean_ranking, color=winterCmap(128), linewidth=4)\n",
    "l2 = figpair[1].fill_between(x, mean_ranking - confidence95, mean_ranking + confidence95, alpha=0.2, color=winterCmap(128))\n",
    "figpair[1].legend([l1[0], l2], [\"uśredniony średni błąd eksp. baseline\", \"przedział ufności 95%\"])\n",
    "figpair[0].savefig(SAVE_PATH +\"baselines-spread-ranks-confidence95.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = merge_tbevents(selectionV[0], \"mean_loss\")[0]\n",
    "values = [merge_tbevents(s, \"mean_loss\")[1] for s in selectionV]\n",
    "values = np.array(values)\n",
    "meanValues = np.mean(values, axis=0)\n",
    "confidence95 = 1.96*np.sqrt(np.var(values, axis=0))\n",
    "\n",
    "figpair = plt.subplots(1,1, figsize=(8,6))\n",
    "l1 = figpair[1].plot(x, meanValues, color=wistiaCmap(128), linewidth=4)\n",
    "l2 = figpair[1].fill_between(x, meanValues - confidence95, meanValues + confidence95, alpha=0.2, color=wistiaCmap(128))\n",
    "\n",
    "x = merge_tbevents(selectionT[0], \"mean_loss\")[0]\n",
    "values = [merge_tbevents(s, \"mean_loss\")[1] for s in selectionT]\n",
    "values = np.array(values)\n",
    "meanValues = np.mean(values, axis=0)\n",
    "confidence95 = 1.96*np.sqrt(np.var(values, axis=0))\n",
    "\n",
    "l3 = figpair[1].plot(x, meanValues, color=winterCmap(128), linewidth=4)\n",
    "l4 = figpair[1].fill_between(x, meanValues - confidence95, meanValues + confidence95, alpha=0.2, color=winterCmap(128))\n",
    "\n",
    "figpair[1].legend([l1[0],l2,l3[0],l4], [\"średni błąd walidacyjny\", \"przedział ufności 95% błędu walidacyjnego\", \"średni błąd uczący\", \"przedział ufności 95% błędu uczącego\"])\n",
    "figpair[0].savefig(SAVE_PATH + \"baselines-spread-confidence95.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-044333\", \"20210424-001214\", \"20210520-202513\", \"20210424-005223\"], pastExperiments),\n",
    "               drawRaw=False,\n",
    "               saveName=\"baseline-improvements.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-044333\", \"20210424-001214\", \"20210520-202513\", \"20210424-005223\"], pastExperiments),\n",
    "               alpha=1.0,\n",
    "               saveName=\"baseline-improvements-ranks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 6. - dodanie dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"20210424-044333\", \"20210521-031306\"], pastExperiments),\n",
    "               drawRaw=False,\n",
    "               saveName=\"baseline-with-dropout.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"20210424-044333\", \"20210521-031306\"], pastExperiments),\n",
    "               saveName=\"baseline-with-dropout-ranks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment 7. - uczenie całej sieci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph(filter_experiments([\"wholeNet\"], pastExperiments),\n",
    "               drawRaw=False,\n",
    "               saveName=\"whole-net.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph(filter_experiments([\"wholeNet\"], pastExperiments),\n",
    "               saveName=\"whole-net-ranks.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
