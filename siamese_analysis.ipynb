{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import scipy.interpolate\n",
    "import re\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"No compatible GPUs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"siamese_logs/batch_03/\" \n",
    "SAVE_PATH = \"/qarr/studia/magister/tekst/graphs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STEP=64\n",
    "winterCmap = matplotlib.cm.get_cmap(\"cool\")\n",
    "wistiaCmap = matplotlib.cm.get_cmap(\"autumn\")\n",
    "matplotlib.rcParams['axes.xmargin'] = 0\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['legend.fontsize'] = 'medium'\n",
    "matplotlib.rcParams['font.size'] = 12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tbevents(dirpath, tag):\n",
    "    eventfiles = [dirpath+\"/\"+x for x in os.listdir(dirpath)]\n",
    "    # Assumption - maximum of data entry per file\n",
    "    xx = []\n",
    "    yy = []\n",
    "    for n, evfile in enumerate(eventfiles):\n",
    "        itr = tf.compat.v1.train.summary_iterator(evfile)\n",
    "        for i in itr:\n",
    "            step = i.step\n",
    "            if i.summary and i.summary.value:\n",
    "                for v in i.summary.value:\n",
    "                    if v.tag == tag:\n",
    "                        f = struct.unpack('f', v.tensor.tensor_content)\n",
    "                        xx.append(step)\n",
    "                        yy.append(f[0])\n",
    "    xx = np.array(xx)\n",
    "    yy = np.array(yy)\n",
    "    order = np.argsort(xx)\n",
    "    xx = xx[order]\n",
    "    yy = yy[order]\n",
    "    return xx, yy\n",
    "\n",
    "def discover_tags(dirpath):\n",
    "    eventfiles = [dirpath+\"/\"+x for x in os.listdir(dirpath)]\n",
    "    tags = set()\n",
    "    for n, evfile in enumerate(eventfiles):\n",
    "        itr = tf.compat.v1.train.summary_iterator(evfile)\n",
    "        for i in itr:\n",
    "            step = i.step\n",
    "            if i.summary and i.summary.value:\n",
    "                for v in i.summary.value:\n",
    "                    tags.add(v.tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(\"/home/zenfur/magister/jupyter/siamese_logs/\")\n",
    "root, dirs, _ = next(os.walk(LOG_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastExperiments = list()\n",
    "for d in dirs:\n",
    "    subdirs = os.listdir(root+\"/\"+d)\n",
    "    if \"validation\" in subdirs:\n",
    "        for sd in subdirs:\n",
    "            pastExperiments.append(root + \"/\" + d + \"/\" + sd)\n",
    "    else:\n",
    "        print(f\"Omitting {d} experiment - no validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chronoOrder = np.argsort([e.split('/')[-2].split('_')[1] for e in pastExperiments])\n",
    "\n",
    "# with open(\"experiments.list\", \"w\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for chord in chronoOrder:\n",
    "#         splits = pastExperiments[chord].split('/')\n",
    "#         name, date = splits[-2].split('_')\n",
    "#         comment = \"\"\n",
    "#         writer.writerow((date, name, pastExperiments[chord], comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, series, weight, points=0):\n",
    "    smoothed = np.zeros(len(series))\n",
    "    smoothed[0] = series[0]\n",
    "    for i in range(1, len(series)):\n",
    "        smoothed[i] = series[i]*(1-weight) + weight*smoothed[i-1]\n",
    "    if points:\n",
    "        spline = cubic_interploation_model=scipy.interpolate.interp1d(x,smoothed,kind=\"cubic\")\n",
    "        xrange = np.linspace(x.min(), x.max(), num=points, endpoint=True, retstep=False, dtype=None, axis=0)\n",
    "        yrange = spline(xrange)\n",
    "        return (xrange, yrange)\n",
    "    else:\n",
    "        return (x, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "    \"\"\"Exponentially weighted moving average; taken from \n",
    "    https://stackoverflow.com/questions/42869495/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm\n",
    "    Has troubles with large datasets due to high power exponents\"\"\"\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "def window_size(alpha, sum_proportion):\n",
    "    # Increases with increased sum_proportion and decreased alpha\n",
    "    # solve (1-alpha)**window_size = (1-sum_proportion) for window_size        \n",
    "    return int(np.log(1-sum_proportion) / np.log(1-alpha))\n",
    "\n",
    "def smooth_ewma(x, series, w):\n",
    "    n = len(series)\n",
    "    # w*1= n/2\n",
    "    # w*0= 1\n",
    "    window = int((n/2-1)*w) + 1\n",
    "    smoothed = numpy_ewma_vectorized_v2(series, window)\n",
    "    return (x, smoothed)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss_graph(experiments, title, legend, drawRaw=True, saveName=None, smoothFn=smooth, smoothParams=None):\n",
    "    exps = []\n",
    "    validationCount = 0\n",
    "    trainCount = 0\n",
    "    \n",
    "    if smoothParams == None:\n",
    "        smoothParams = [0.8]\n",
    "    \n",
    "    for ex in experiments:\n",
    "        s = re.sub(r\".*//\", \"\" , ex)\n",
    "        nameString, dsType = s.split(\"/\")\n",
    "        expName = re.sub(r\"_.*$\", \"\", nameString)\n",
    "        exps.append((dsType, expName, ex))\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "    #fig.tight_layout()\n",
    "    axs.set_title(title)\n",
    "    axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "    defaultLegend = []\n",
    "    for dsType, expName, expPath in exps:\n",
    "        print(dsType, expName, expPath)\n",
    "        if dsType == \"validation\":\n",
    "            color = wistiaCmap(256-COLOR_STEP*validationCount)\n",
    "            defaultLegend.append(expName + \" - valid\")\n",
    "            validationCount += 1\n",
    "        else:\n",
    "            color = winterCmap(COLOR_STEP*trainCount)\n",
    "            defaultLegend.append(expName + \" - train\")\n",
    "            trainCount += 1\n",
    "        \n",
    "        xx, yy = merge_tbevents(expPath, \"mean_loss\")\n",
    "        if drawRaw:\n",
    "            axs.plot(xx,yy, alpha=0.3, color=color)\n",
    "        axs.plot(*smoothFn(xx, yy, *smoothParams), alpha=1, color=color)\n",
    "\n",
    "    axs.set_xlabel(\"numer epoki\")\n",
    "    axs.set_ylabel(\"uśredniona funkcja straty z epoki\")\n",
    "    if legend is not None:\n",
    "        axs.legend(legend)\n",
    "    else:\n",
    "        axs.legend(defaultLegend)\n",
    "    if saveName:\n",
    "        fig.savefig(SAVE_PATH + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_ranking_graph(experiments, title, legend, alpha=0.5, saveName=None):\n",
    "    exps = []\n",
    "    validationCount = 0\n",
    "    trainCount = 0\n",
    "        \n",
    "    for ex in experiments:\n",
    "        s = re.sub(r\".*//\", \"\" , ex)\n",
    "        nameString, dsType = s.split(\"/\")\n",
    "        expName = re.sub(r\"_.*$\", \"\", nameString)\n",
    "        if dsType == \"validation\":\n",
    "            print(f\"Skipping {ex}, no data\")\n",
    "            continue\n",
    "        exps.append((dsType, expName, ex))\n",
    "    \n",
    "    defaultLegend = []\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "    #fig.tight_layout()\n",
    "    axs.set_title(title)\n",
    "    axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "    \n",
    "    for dsType, expName, expPath in exps:\n",
    "        print(dsType, expName, expPath)\n",
    "        color = winterCmap(COLOR_STEP*trainCount)\n",
    "        defaultLegend.append(expName)\n",
    "        trainCount += 1\n",
    "        \n",
    "        xx, yy = merge_tbevents(expPath, \"rank_normalised\")\n",
    "        axs.plot(xx,yy, alpha=alpha, color=color)\n",
    "\n",
    "    axs.set_xlabel(\"numer epoki\")\n",
    "    axs.set_ylabel(\"znormalizowany ranking\")\n",
    "    if not legend:\n",
    "        axs.legend(defaultLegend)\n",
    "    else:\n",
    "        axs.legend(legend)\n",
    "    if saveName:\n",
    "        fig.savefig(SAVE_PATH + saveName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxt, yyt = merge_tbevents(pastExperiments[1], \"mean_loss\")\n",
    "xxv, yyv = merge_tbevents(pastExperiments[0], \"mean_loss\")\n",
    "print(pastExperiments[0])\n",
    "fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "fig.tight_layout()\n",
    "axs.set_title(\"Uśredniona funkcja straty w przebiegu uczenia eks. 'baseline'\")\n",
    "axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "axs.plot(xxt,yyt, alpha=0.5, color=winterCmap(0))\n",
    "axs.plot(*smooth_ewma(xxt, yyt,0.3), alpha=1, color=winterCmap(0))\n",
    "\n",
    "axs.plot(xxv,yyv, alpha=0.5, color=wistiaCmap(256))\n",
    "axs.plot(*smooth_ewma(xxv, yyv,0.3), alpha=1, color=wistiaCmap(256))\n",
    "axs.set_xlabel(\"numer epoki\")\n",
    "axs.set_ylabel(\"funkcja straty z epoki\")\n",
    "#axs.legend(\"a\",\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxt, yyt = merge_tbevents(pastExperiments[1], \"mean_loss\")\n",
    "xxv, yyv = merge_tbevents(pastExperiments[0], \"mean_loss\")\n",
    "print(pastExperiments[0])\n",
    "fig, axs = plt.subplots(1,1, figsize=(8,6))\n",
    "fig.tight_layout()\n",
    "axs.set_title(\"Uśredniona funkcja straty w przebiegu uczenia eks. 'baseline'\")\n",
    "axs.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(5))\n",
    "axs.plot(xxt,yyt, alpha=0.5, color=winterCmap(0))\n",
    "axs.plot(*smooth(xxt, yyt,0.8), alpha=1, color=winterCmap(0))\n",
    "\n",
    "axs.plot(xxv,yyv, alpha=0.5, color=wistiaCmap(256))\n",
    "axs.plot(*smooth(xxv, yyv,0.8), alpha=1, color=wistiaCmap(256))\n",
    "axs.set_xlabel(\"numer epoki\")\n",
    "axs.set_ylabel(\"funkcja straty z epoki\")\n",
    "axs.legend([\"błąd zbioru trenującego\", \"wygładzony błąd zb. trenującego\", \"błąd zbioru walidacyjnego\", \"wygładzony błąd zb. walidacyjnego\"])\n",
    "fig.savefig(SAVE_PATH + \"baseline_meanError_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph([pastExperiments[1], pastExperiments[0]], \n",
    "                \"Uśredniona funkcja straty eksperymentu 'baseline'\",\n",
    "               [\"błąd zbioru trenującego\", \"wygładzony błąd zb. trenującego\", \"błąd zbioru walidacyjnego\", \"wygładzony błąd zb. walidacyjnego\"],\n",
    "               saveName=\"baseline_meanError_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph([pastExperiments[1]], \"Znormalizowany ranking eksperymentu 'baseline'\", [\"ranking baseline\"], saveName=\"baseline_normRanking_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tbevents(pastExperiments[1], \"rank_normalised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph([pastExperiments[1], pastExperiments[0], pastExperiments[-1], pastExperiments[-2],  pastExperiments[7], pastExperiments[6]], \n",
    "                \"\",\n",
    "               ['baseline - training', 'baseline - valid.', 'output norm 3 - train', 'output norm 3 - valid.', 'output norm 2 - train', 'output norm 2 - valid.'],\n",
    "               drawRaw=False,\n",
    "               saveName=\"output-normalisations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph([pastExperiments[1], pastExperiments[-1],   pastExperiments[7]], \n",
    "                \"\",\n",
    "               ['baseline', 'output norm 3', 'output norm 2'],\n",
    "               alpha=1,\n",
    "               saveName=\"output-normalisations-ranks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_graph([pastExperiments[i] for i in [1,0,5,4,9,8,13,12]], \n",
    "                \"\",\n",
    "               None,\n",
    "               drawRaw=False,\n",
    "               saveName=\"different-sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph([pastExperiments[i] for i in [1,0,5,4,9,8,13,12]], \n",
    "                \"\",\n",
    "               None,\n",
    "               drawRaw=False,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,p in enumerate(pastExperiments):\n",
    "    print(i ,p )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ranking_graph([pastExperiments[i] for i in [1,0,5,4,9,8,13,12]], \n",
    "                \"\",\n",
    "               None,\n",
    "               alpha=1,\n",
    "               saveName=\"different-sizes-ranks\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
